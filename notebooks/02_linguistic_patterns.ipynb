{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports and NLP tools\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset FutureMa/EvasionBench...\")\n",
    "ds = load_dataset(\"FutureMa/EvasionBench\")\n",
    "if isinstance(ds, dict):\n",
    "    ds = ds[list(ds.keys())[0]]\n",
    "df = ds.to_pandas()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Load spaCy model (install if missing)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    print(\"spaCy model en_core_web_sm not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Quick sample\n",
    "print(df[\"eva4b_label\"].value_counts())\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f51bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Analysis - Vocabulary Size\n",
    "print(\"=\" * 60)\n",
    "print(\"LEXICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_vocabulary(texts):\n",
    "    \"\"\"Extract vocabulary from a series of texts.\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "        all_words.extend(words)\n",
    "    return set(all_words), Counter(all_words)\n",
    "\n",
    "# Vocabulary by category\n",
    "vocab_by_label = {}\n",
    "for label in df['eva4b_label'].unique():\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    vocab, word_counts = get_vocabulary(texts)\n",
    "    vocab_by_label[label] = {'vocab': vocab, 'counts': word_counts, 'size': len(vocab)}\n",
    "\n",
    "print(\"\\nVocabulary size by evasion category:\")\n",
    "for label, data in sorted(vocab_by_label.items(), key=lambda x: x[1]['size'], reverse=True):\n",
    "    print(f\"  {label}: {data['size']:,} unique words\")\n",
    "\n",
    "# Overall vocabulary\n",
    "all_vocab, all_counts = get_vocabulary(df['answer'])\n",
    "print(f\"\\nTotal vocabulary (all answers): {len(all_vocab):,} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9d9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Frequent N-grams by Category\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def get_top_ngrams(texts, n=1, top_k=20):\n",
    "    \"\"\"Extract top k n-grams from texts.\"\"\"\n",
    "    vec = CountVectorizer(ngram_range=(n, n), stop_words='english', max_features=10000)\n",
    "    X = vec.fit_transform(texts)\n",
    "    sums = X.sum(axis=0)\n",
    "    data = [(word, sums[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(data, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP UNIGRAMS BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    top_unigrams = get_top_ngrams(texts, n=1, top_k=15)\n",
    "    print(f\"\\n{label.upper()} - Top 15 unigrams:\")\n",
    "    for i, (word, count) in enumerate(top_unigrams, 1):\n",
    "        print(f\"  {i:2d}. {word}: {count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP BIGRAMS BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    top_bigrams = get_top_ngrams(texts, n=2, top_k=10)\n",
    "    print(f\"\\n{label.upper()} - Top 10 bigrams:\")\n",
    "    for i, (phrase, count) in enumerate(top_bigrams, 1):\n",
    "        print(f\"  {i:2d}. {phrase}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Analysis - Distinguishing Terms\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TF-IDF ANALYSIS - DISTINCTIVE TERMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_top_tfidf(texts, top_k=20):\n",
    "    \"\"\"Extract top TF-IDF terms.\"\"\"\n",
    "    vec = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=10000)\n",
    "    X = vec.fit_transform(texts)\n",
    "    mean_tfidf = X.mean(axis=0)\n",
    "    data = [(word, mean_tfidf[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(data, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    top_tfidf = get_top_tfidf(texts, top_k=15)\n",
    "    print(f\"\\n{label.upper()} - Top 15 TF-IDF terms:\")\n",
    "    for i, (term, score) in enumerate(top_tfidf, 1):\n",
    "        print(f\"  {i:2d}. {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6619865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability Metrics\n",
    "import textstat\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"READABILITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate readability scores for each answer\n",
    "df['flesch_ease'] = df['answer'].apply(textstat.flesch_reading_ease)\n",
    "df['flesch_kincaid'] = df['answer'].apply(textstat.flesch_kincaid_grade)\n",
    "df['smog'] = df['answer'].apply(textstat.smog_index)\n",
    "\n",
    "# Statistics by label\n",
    "print(\"\\nReadability scores by evasion category:\")\n",
    "readability_stats = df.groupby('eva4b_label').agg({\n",
    "    'flesch_ease': ['mean', 'std', 'median'],\n",
    "    'flesch_kincaid': ['mean', 'std', 'median'],\n",
    "    'smog': ['mean', 'std', 'median']\n",
    "}).round(2)\n",
    "\n",
    "print(readability_stats)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"  Flesch Reading Ease: Higher = easier to read (0-100 scale)\")\n",
    "print(\"  Flesch-Kincaid Grade: US grade level needed to understand\")\n",
    "print(\"  SMOG Index: Years of education needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385afd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['flesch_ease', 'flesch_kincaid', 'smog']\n",
    "titles = ['Flesch Reading Ease', 'Flesch-Kincaid Grade', 'SMOG Index']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    df.boxplot(column=metric, by='eva4b_label', ax=ax)\n",
    "    ax.set_xlabel('Evasion Label')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "plt.suptitle('Readability Metrics by Evasion Category', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/figures/02_readability_by_label.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e865bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis & Hedging Word Counts\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Hedging and certainty lexicons\n",
    "hedging_words = [\n",
    "    'might', 'could', 'possibly', 'maybe', 'perhaps', 'potentially',\n",
    "    'approximately', 'roughly', 'about', 'around', 'somewhat',\n",
    "    'not sure', 'hard to say', 'depends', 'various', 'a bit',\n",
    "    'to some extent', 'uncertain', 'unsure', 'not certain'\n",
    "]\n",
    "\n",
    "certainty_words = [\n",
    "    'definitely', 'certainly', 'exactly', 'precisely', 'absolutely',\n",
    "    'clearly', 'undoubtedly', 'undoubtedly', 'undisputably',\n",
    "    'certain', 'sure', 'guarantee', 'confirm', 'assure'\n",
    "]\n",
    "\n",
    "# Sentiment analysis\n",
    "df['sentiment_polarity'] = df['answer'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['sentiment_subjectivity'] = df['answer'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "# Hedging word counts\n",
    "def count_hedge_words(text):\n",
    "    words = [w.lower() for w in text.split() if w.lower() not in stop_words]\n",
    "    return sum(1 for w in words if w in hedging_words)\n",
    "\n",
    "def count_certainty_words(text):\n",
    "    words = [w.lower() for w in text.split() if w.lower() not in stop_words]\n",
    "    return sum(1 for w in words if w in certainty_words)\n",
    "\n",
    "df['hedge_word_count'] = df['answer'].apply(count_hedge_words)\n",
    "df['certainty_word_count'] = df['answer'].apply(count_certainty_words)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT & HEDGING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sentiment stats by label\n",
    "sent_stats = df.groupby('eva4b_label').agg({\n",
    "    'sentiment_polarity': ['mean', 'std'],\n",
    "    'sentiment_subjectivity': ['mean', 'std'],\n",
    "    'hedge_word_count': ['mean', 'std'],\n",
    "    'certainty_word_count': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nSentiment and hedging statistics by label:\")\n",
    "print(sent_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tag Visualization (if spaCy is loaded)\n",
    "if nlp:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PART-OF-SPEECH ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def get_pos_proportions(text):\n",
    "        \"\"\"Calculate POS tag proportions.\"\"\"\n",
    "        doc = nlp(text)\n",
    "        pos_counts = {}\n",
    "        for token in doc:\n",
    "            pos = token.pos_\n",
    "            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "        total = sum(pos_counts.values())\n",
    "        return {pos: count/total for pos, count in pos_counts.items()}\n",
    "    \n",
    "    # Sample and analyze POS for each label\n",
    "    pos_by_label = {}\n",
    "    for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "        samples = df[df['eva4b_label'] == label]['answer'].head(100)\n",
    "        all_pos = []\n",
    "        for text in samples:\n",
    "            all_pos.append(get_pos_proportions(text))\n",
    "        \n",
    "        # Average across samples\n",
    "        avg_pos = {pos: np.mean([p.get(pos, 0) for p in all_pos]) \n",
    "                     for pos in ['ADJ', 'ADV', 'NOUN', 'VERB']}\n",
    "        pos_by_label[label] = avg_pos\n",
    "    \n",
    "    print(\"\\nAverage POS proportions by label:\")\n",
    "    for label, pos_dict in pos_by_label.items():\n",
    "        print(f\"\\n{label.upper()}:\")\n",
    "        for pos, prop in sorted(pos_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {pos}: {prop:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è spaCy model not loaded. Install with: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (if spaCy is loaded)\n",
    "if nlp:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NAMED ENTITY RECOGNITION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def count_entities(text):\n",
    "        \"\"\"Count entities by type.\"\"\"\n",
    "        doc = nlp(text)\n",
    "        entity_types = ['ORG', 'MONEY', 'PERCENT', 'NUMBER', 'GPE', 'PERSON']\n",
    "        counts = {etype: 0 for etype in entity_types}\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in entity_types:\n",
    "                counts[ent.label_] += 1\n",
    "        return counts\n",
    "    \n",
    "    # Sample NER analysis per label\n",
    "    for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "        samples = df[df['eva4b_label'] == label]['answer'].head(200)\n",
    "        all_counts = []\n",
    "        for text in samples:\n",
    "            all_counts.append(count_entities(text))\n",
    "        \n",
    "        # Average counts\n",
    "        avg_counts = {etype: np.mean([c.get(etype, 0) for c in all_counts]) \n",
    "                      for etype in ['ORG', 'MONEY', 'PERCENT', 'NUMBER']}\n",
    "        print(f\"\\n{label.upper()} - Average entity counts (n=200):\")\n",
    "        for etype, count in sorted(avg_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {etype}: {count:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è spaCy model not loaded. Install with: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c61d9",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Lexical & N-gram Patterns\n",
    "- Fully evasive answers have [distinctive unigrams/bigrams compared to direct]\n",
    "- TF-IDF reveals [key terms distinguishing each category]\n",
    "- Vocabulary size [similar/different across categories]\n",
    "\n",
    "### Readability\n",
    "- [Higher/Lower] readability for evasive (easier/harder to read)\n",
    "- Suggests [more complex/simple language patterns]\n",
    "\n",
    "### Sentiment & Hedging\n",
    "- Evasive answers show [more/less] hedging words (XX.X vs Y.Y per answer)\n",
    "- Certainty markers [more/less] frequent in direct answers\n",
    "- Sentiment polarity [biased toward] for certain labels\n",
    "\n",
    "### POS & Entities (if spaCy loaded)\n",
    "- Evasive answers use more/less adjectives/adverbs [check after execution]\n",
    "- Entity density [similar/different] across categories\n",
    "\n",
    "### Linguistic Evasion Markers Identified\n",
    "Top potential evasion markers:\n",
    "1. [List from hedging/certainty analysis]\n",
    "2. [Other features from N-grams]\n",
    "3. [Specific phrases from manual inspection]\n",
    "\n",
    "### Next Steps\n",
    "- Use identified features for Notebook 4 (traditional ML baselines)\n",
    "- Validate linguistic markers with model interpretability (Notebook 9)\n",
    "- Develop rule-based classifier from these markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba675ca",
   "metadata": {},
   "source": [
    "# 02 - Linguistic Patterns\n",
    "\n",
    "**Objectives:**\n",
    "- Lexical and syntactic analysis by label\n",
    "- Readability, POS, NER, hedging analysis\n",
    "- Produce feature candidates for models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
