{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2dfcc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset FutureMa/EvasionBench...\n",
      "Dataset shape: (16726, 4)\n",
      "spaCy model en_core_web_sm not found. Run: python -m spacy download en_core_web_sm\n",
      "direct           8749\n",
      "intermediate     7359\n",
      "fully_evasive     618\n",
      "Name: eva4b_label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>eva4b_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12768</th>\n",
       "      <td>e642181a7321aedc077ee9a40ca0b43c</td>\n",
       "      <td>So I just wondered if you could give us a sens...</td>\n",
       "      <td>The first one, Joel. We have, we took the pric...</td>\n",
       "      <td>intermediate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15807</th>\n",
       "      <td>0e14ab0cba14820c468deb83050f3b3e</td>\n",
       "      <td>Tissue distribution is becoming an emerging th...</td>\n",
       "      <td>Yeah, thanks for the question. It's a very imp...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15246</th>\n",
       "      <td>9ee3b1d69b630a2ad7a45128292c7629</td>\n",
       "      <td>Okay, that's very helpful. And then John, a qu...</td>\n",
       "      <td>Well, the delay is a tool that's going to be d...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2678</th>\n",
       "      <td>b122f4037a50c7f2b818dc8f3903d4a5</td>\n",
       "      <td>All right. I assume it -- well, I assume it go...</td>\n",
       "      <td>Yeah, I mean I can't give you any firm guidanc...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7085</th>\n",
       "      <td>cd51840ba393ac289975c27df48681ad</td>\n",
       "      <td>As I look at the North America business, the i...</td>\n",
       "      <td>Sure. So, the increase Jeff was in several dif...</td>\n",
       "      <td>intermediate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    uid  \\\n",
       "12768  e642181a7321aedc077ee9a40ca0b43c   \n",
       "15807  0e14ab0cba14820c468deb83050f3b3e   \n",
       "15246  9ee3b1d69b630a2ad7a45128292c7629   \n",
       "2678   b122f4037a50c7f2b818dc8f3903d4a5   \n",
       "7085   cd51840ba393ac289975c27df48681ad   \n",
       "\n",
       "                                                question  \\\n",
       "12768  So I just wondered if you could give us a sens...   \n",
       "15807  Tissue distribution is becoming an emerging th...   \n",
       "15246  Okay, that's very helpful. And then John, a qu...   \n",
       "2678   All right. I assume it -- well, I assume it go...   \n",
       "7085   As I look at the North America business, the i...   \n",
       "\n",
       "                                                  answer   eva4b_label  \n",
       "12768  The first one, Joel. We have, we took the pric...  intermediate  \n",
       "15807  Yeah, thanks for the question. It's a very imp...        direct  \n",
       "15246  Well, the delay is a tool that's going to be d...        direct  \n",
       "2678   Yeah, I mean I can't give you any firm guidanc...        direct  \n",
       "7085   Sure. So, the increase Jeff was in several dif...  intermediate  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup: imports and NLP tools\n",
    "import sys, pathlib, os\n",
    "# Add project root to sys.path\n",
    "proj_root = pathlib.Path('..').resolve()\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "try:\n",
    "    from src.utils import set_seed\n",
    "except Exception as e:\n",
    "    print('Warning: could not import src.utils.set_seed:', e)\n",
    "    set_seed = lambda _: None\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset FutureMa/EvasionBench...\")\n",
    "ds = load_dataset(\"FutureMa/EvasionBench\")\n",
    "if isinstance(ds, dict):\n",
    "    ds = ds[list(ds.keys())[0]]\n",
    "df = ds.to_pandas()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Load spaCy model (install if missing)\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    print(\"spaCy model en_core_web_sm not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Quick sample\n",
    "print(df[\"eva4b_label\"].value_counts())\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f51bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEXICAL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Vocabulary size by evasion category:\n",
      "  direct: 19,777 unique words\n",
      "  intermediate: 19,677 unique words\n",
      "  fully_evasive: 4,043 unique words\n",
      "\n",
      "Total vocabulary (all answers): 25,933 unique words\n"
     ]
    }
   ],
   "source": [
    "# Lexical Analysis - Vocabulary Size\n",
    "print(\"=\" * 60)\n",
    "print(\"LEXICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_vocabulary(texts):\n",
    "    \"\"\"Extract vocabulary from a series of texts.\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
    "        all_words.extend(words)\n",
    "    return set(all_words), Counter(all_words)\n",
    "\n",
    "# Vocabulary by category\n",
    "vocab_by_label = {}\n",
    "for label in df['eva4b_label'].unique():\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    vocab, word_counts = get_vocabulary(texts)\n",
    "    vocab_by_label[label] = {'vocab': vocab, 'counts': word_counts, 'size': len(vocab)}\n",
    "\n",
    "print(\"\\nVocabulary size by evasion category:\")\n",
    "for label, data in sorted(vocab_by_label.items(), key=lambda x: x[1]['size'], reverse=True):\n",
    "    print(f\"  {label}: {data['size']:,} unique words\")\n",
    "\n",
    "# Overall vocabulary\n",
    "all_vocab, all_counts = get_vocabulary(df['answer'])\n",
    "print(f\"\\nTotal vocabulary (all answers): {len(all_vocab):,} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33d9d9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TOP UNIGRAMS BY CATEGORY\n",
      "============================================================\n",
      "\n",
      "DIRECT - Top 15 unigrams:\n",
      "   1. think: 9,739\n",
      "   2. year: 6,588\n",
      "   3. ve: 5,797\n",
      "   4. going: 4,922\n",
      "   5. really: 4,916\n",
      "   6. just: 4,483\n",
      "   7. quarter: 4,304\n",
      "   8. business: 3,492\n",
      "   9. yes: 3,169\n",
      "  10. ll: 3,124\n",
      "  11. right: 3,040\n",
      "  12. million: 2,698\n",
      "  13. good: 2,640\n",
      "  14. lot: 2,566\n",
      "  15. like: 2,527\n",
      "\n",
      "INTERMEDIATE - Top 15 unigrams:\n",
      "   1. think: 10,202\n",
      "   2. year: 6,158\n",
      "   3. going: 5,345\n",
      "   4. ve: 5,278\n",
      "   5. really: 4,737\n",
      "   6. just: 4,270\n",
      "   7. quarter: 3,886\n",
      "   8. business: 3,662\n",
      "   9. ll: 3,527\n",
      "  10. right: 3,043\n",
      "  11. know: 2,951\n",
      "  12. market: 2,892\n",
      "  13. look: 2,886\n",
      "  14. continue: 2,676\n",
      "  15. growth: 2,649\n",
      "\n",
      "FULLY_EVASIVE - Top 15 unigrams:\n",
      "   1. think: 416\n",
      "   2. don: 290\n",
      "   3. going: 245\n",
      "   4. just: 208\n",
      "   5. really: 195\n",
      "   6. ll: 185\n",
      "   7. ve: 181\n",
      "   8. quarter: 165\n",
      "   9. year: 165\n",
      "  10. time: 153\n",
      "  11. right: 152\n",
      "  12. like: 138\n",
      "  13. say: 133\n",
      "  14. yes: 127\n",
      "  15. know: 120\n",
      "\n",
      "============================================================\n",
      "TOP BIGRAMS BY CATEGORY\n",
      "============================================================\n",
      "\n",
      "DIRECT - Top 10 bigrams:\n",
      "   1. little bit: 1,519\n",
      "   2. ve got: 875\n",
      "   3. ve seen: 734\n",
      "   4. fourth quarter: 582\n",
      "   5. second quarter: 519\n",
      "   6. long term: 490\n",
      "   7. going forward: 489\n",
      "   8. second half: 480\n",
      "   9. year year: 462\n",
      "  10. half year: 430\n",
      "\n",
      "INTERMEDIATE - Top 10 bigrams:\n",
      "   1. little bit: 1,193\n",
      "   2. ve got: 737\n",
      "   3. ve seen: 666\n",
      "   4. long term: 610\n",
      "   5. going forward: 529\n",
      "   6. fourth quarter: 487\n",
      "   7. year year: 484\n",
      "   8. second quarter: 467\n",
      "   9. make sure: 430\n",
      "  10. half year: 387\n",
      "\n",
      "FULLY_EVASIVE - Top 10 bigrams:\n",
      "   1. don think: 41\n",
      "   2. don know: 35\n",
      "   3. don want: 34\n",
      "   4. little bit: 26\n",
      "   5. fourth quarter: 26\n",
      "   6. ve got: 23\n",
      "   7. make sure: 21\n",
      "   8. long term: 21\n",
      "   9. going forward: 19\n",
      "  10. think going: 19\n"
     ]
    }
   ],
   "source": [
    "# Most Frequent N-grams by Category\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "def get_top_ngrams(texts, n=1, top_k=20):\n",
    "    \"\"\"Extract top k n-grams from texts.\"\"\"\n",
    "    vec = CountVectorizer(ngram_range=(n, n), stop_words='english', max_features=10000)\n",
    "    X = vec.fit_transform(texts)\n",
    "    sums = X.sum(axis=0)\n",
    "    data = [(word, sums[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(data, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP UNIGRAMS BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    top_unigrams = get_top_ngrams(texts, n=1, top_k=15)\n",
    "    print(f\"\\n{label.upper()} - Top 15 unigrams:\")\n",
    "    for i, (word, count) in enumerate(top_unigrams, 1):\n",
    "        print(f\"  {i:2d}. {word}: {count:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP BIGRAMS BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    top_bigrams = get_top_ngrams(texts, n=2, top_k=10)\n",
    "    print(f\"\\n{label.upper()} - Top 10 bigrams:\")\n",
    "    for i, (phrase, count) in enumerate(top_bigrams, 1):\n",
    "        print(f\"  {i:2d}. {phrase}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff6e35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TF-IDF ANALYSIS - DISTINCTIVE TERMS\n",
      "============================================================\n",
      "\n",
      "DIRECT - Top 15 TF-IDF terms:\n",
      "   1. think: 0.0353\n",
      "   2. year: 0.0291\n",
      "   3. quarter: 0.0241\n",
      "   4. ve: 0.0234\n",
      "   5. yes: 0.0228\n",
      "   6. going: 0.0213\n",
      "   7. really: 0.0210\n",
      "   8. million: 0.0206\n",
      "   9. just: 0.0194\n",
      "  10. business: 0.0173\n",
      "  11. right: 0.0172\n",
      "  12. yeah: 0.0168\n",
      "  13. ll: 0.0158\n",
      "  14. good: 0.0140\n",
      "  15. don: 0.0138\n",
      "\n",
      "INTERMEDIATE - Top 15 TF-IDF terms:\n",
      "   1. think: 0.0386\n",
      "   2. year: 0.0295\n",
      "   3. going: 0.0248\n",
      "   4. ve: 0.0238\n",
      "   5. quarter: 0.0234\n",
      "   6. really: 0.0226\n",
      "   7. just: 0.0205\n",
      "   8. business: 0.0199\n",
      "   9. ll: 0.0189\n",
      "  10. know: 0.0174\n",
      "  11. right: 0.0173\n",
      "  12. market: 0.0167\n",
      "  13. look: 0.0163\n",
      "  14. growth: 0.0162\n",
      "  15. continue: 0.0158\n",
      "\n",
      "FULLY_EVASIVE - Top 15 TF-IDF terms:\n",
      "   1. think: 0.0320\n",
      "   2. don: 0.0299\n",
      "   3. ll: 0.0224\n",
      "   4. going: 0.0222\n",
      "   5. really: 0.0194\n",
      "   6. just: 0.0192\n",
      "   7. right: 0.0190\n",
      "   8. time: 0.0179\n",
      "   9. quarter: 0.0172\n",
      "  10. yeah: 0.0169\n",
      "  11. year: 0.0167\n",
      "  12. ve: 0.0161\n",
      "  13. question: 0.0155\n",
      "  14. number: 0.0151\n",
      "  15. say: 0.0150\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Analysis - Distinguishing Terms\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TF-IDF ANALYSIS - DISTINCTIVE TERMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def get_top_tfidf(texts, top_k=20):\n",
    "    \"\"\"Extract top TF-IDF terms.\"\"\"\n",
    "    vec = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_features=10000)\n",
    "    X = vec.fit_transform(texts)\n",
    "    mean_tfidf = X.mean(axis=0)\n",
    "    data = [(word, mean_tfidf[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    return sorted(data, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    texts = df[df['eva4b_label'] == label]['answer']\n",
    "    top_tfidf = get_top_tfidf(texts, top_k=15)\n",
    "    print(f\"\\n{label.upper()} - Top 15 TF-IDF terms:\")\n",
    "    for i, (term, score) in enumerate(top_tfidf, 1):\n",
    "        print(f\"  {i:2d}. {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6619865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "READABILITY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Readability scores by evasion category:\n",
      "              flesch_ease               flesch_kincaid                smog  \\\n",
      "                     mean    std median           mean   std median   mean   \n",
      "eva4b_label                                                                  \n",
      "direct              63.93  14.36  63.83           9.09  3.47   8.96  11.07   \n",
      "fully_evasive       69.63  18.19  68.95           7.91  4.13   7.91   9.83   \n",
      "intermediate        60.96  11.92  61.69           9.87  3.00   9.56  11.74   \n",
      "\n",
      "                            \n",
      "                std median  \n",
      "eva4b_label                 \n",
      "direct         2.68  11.21  \n",
      "fully_evasive  3.43  10.13  \n",
      "intermediate   2.16  11.70  \n",
      "\n",
      "üìä Interpretation:\n",
      "  Flesch Reading Ease: Higher = easier to read (0-100 scale)\n",
      "  Flesch-Kincaid Grade: US grade level needed to understand\n",
      "  SMOG Index: Years of education needed\n"
     ]
    }
   ],
   "source": [
    "# Readability Metrics\n",
    "import textstat\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"READABILITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate readability scores for each answer\n",
    "df['flesch_ease'] = df['answer'].apply(textstat.flesch_reading_ease)\n",
    "df['flesch_kincaid'] = df['answer'].apply(textstat.flesch_kincaid_grade)\n",
    "df['smog'] = df['answer'].apply(textstat.smog_index)\n",
    "\n",
    "# Statistics by label\n",
    "print(\"\\nReadability scores by evasion category:\")\n",
    "readability_stats = df.groupby('eva4b_label').agg({\n",
    "    'flesch_ease': ['mean', 'std', 'median'],\n",
    "    'flesch_kincaid': ['mean', 'std', 'median'],\n",
    "    'smog': ['mean', 'std', 'median']\n",
    "}).round(2)\n",
    "\n",
    "print(readability_stats)\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"  Flesch Reading Ease: Higher = easier to read (0-100 scale)\")\n",
    "print(\"  Flesch-Kincaid Grade: US grade level needed to understand\")\n",
    "print(\"  SMOG Index: Years of education needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385afd09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Readability Visualization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      4\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflesch_ease\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflesch_kincaid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmog\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m titles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlesch Reading Ease\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlesch-Kincaid Grade\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSMOG Index\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Readability Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.makedirs('notebooks/figures', exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['flesch_ease', 'flesch_kincaid', 'smog']\n",
    "titles = ['Flesch Reading Ease', 'Flesch-Kincaid Grade', 'SMOG Index']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    df.boxplot(column=metric, by='eva4b_label', ax=ax)\n",
    "    ax.set_xlabel('Evasion Label')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(title)\n",
    "    \n",
    "plt.suptitle('Readability Metrics by Evasion Category', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/figures/02_readability_by_label.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e865bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis & Hedging Word Counts\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Hedging and certainty lexicons\n",
    "hedging_words = [\n",
    "    'might', 'could', 'possibly', 'maybe', 'perhaps', 'potentially',\n",
    "    'approximately', 'roughly', 'about', 'around', 'somewhat',\n",
    "    'not sure', 'hard to say', 'depends', 'various', 'a bit',\n",
    "    'to some extent', 'uncertain', 'unsure', 'not certain'\n",
    "]\n",
    "\n",
    "certainty_words = [\n",
    "    'definitely', 'certainly', 'exactly', 'precisely', 'absolutely',\n",
    "    'clearly', 'undoubtedly', 'undoubtedly', 'undisputably',\n",
    "    'certain', 'sure', 'guarantee', 'confirm', 'assure'\n",
    "]\n",
    "\n",
    "# Sentiment analysis\n",
    "df['sentiment_polarity'] = df['answer'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['sentiment_subjectivity'] = df['answer'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "# Hedging word counts\n",
    "def count_hedge_words(text):\n",
    "    words = [w.lower() for w in text.split() if w.lower() not in stop_words]\n",
    "    return sum(1 for w in words if w in hedging_words)\n",
    "\n",
    "def count_certainty_words(text):\n",
    "    words = [w.lower() for w in text.split() if w.lower() not in stop_words]\n",
    "    return sum(1 for w in words if w in certainty_words)\n",
    "\n",
    "df['hedge_word_count'] = df['answer'].apply(count_hedge_words)\n",
    "df['certainty_word_count'] = df['answer'].apply(count_certainty_words)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT & HEDGING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sentiment stats by label\n",
    "sent_stats = df.groupby('eva4b_label').agg({\n",
    "    'sentiment_polarity': ['mean', 'std'],\n",
    "    'sentiment_subjectivity': ['mean', 'std'],\n",
    "    'hedge_word_count': ['mean', 'std'],\n",
    "    'certainty_word_count': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\nSentiment and hedging statistics by label:\")\n",
    "print(sent_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tag Visualization (if spaCy is loaded)\n",
    "if nlp:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PART-OF-SPEECH ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def get_pos_proportions(text):\n",
    "        \"\"\"Calculate POS tag proportions.\"\"\"\n",
    "        doc = nlp(text)\n",
    "        pos_counts = {}\n",
    "        for token in doc:\n",
    "            pos = token.pos_\n",
    "            pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
    "        total = sum(pos_counts.values())\n",
    "        return {pos: count/total for pos, count in pos_counts.items()}\n",
    "    \n",
    "    # Sample and analyze POS for each label\n",
    "    pos_by_label = {}\n",
    "    for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "        samples = df[df['eva4b_label'] == label]['answer'].head(100)\n",
    "        all_pos = []\n",
    "        for text in samples:\n",
    "            all_pos.append(get_pos_proportions(text))\n",
    "        \n",
    "        # Average across samples\n",
    "        avg_pos = {pos: np.mean([p.get(pos, 0) for p in all_pos]) \n",
    "                     for pos in ['ADJ', 'ADV', 'NOUN', 'VERB']}\n",
    "        pos_by_label[label] = avg_pos\n",
    "    \n",
    "    print(\"\\nAverage POS proportions by label:\")\n",
    "    for label, pos_dict in pos_by_label.items():\n",
    "        print(f\"\\n{label.upper()}:\")\n",
    "        for pos, prop in sorted(pos_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {pos}: {prop:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è spaCy model not loaded. Install with: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246e07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (if spaCy is loaded)\n",
    "if nlp:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NAMED ENTITY RECOGNITION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    def count_entities(text):\n",
    "        \"\"\"Count entities by type.\"\"\"\n",
    "        doc = nlp(text)\n",
    "        entity_types = ['ORG', 'MONEY', 'PERCENT', 'NUMBER', 'GPE', 'PERSON']\n",
    "        counts = {etype: 0 for etype in entity_types}\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in entity_types:\n",
    "                counts[ent.label_] += 1\n",
    "        return counts\n",
    "    \n",
    "    # Sample NER analysis per label\n",
    "    for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "        samples = df[df['eva4b_label'] == label]['answer'].head(200)\n",
    "        all_counts = []\n",
    "        for text in samples:\n",
    "            all_counts.append(count_entities(text))\n",
    "        \n",
    "        # Average counts\n",
    "        avg_counts = {etype: np.mean([c.get(etype, 0) for c in all_counts]) \n",
    "                      for etype in ['ORG', 'MONEY', 'PERCENT', 'NUMBER']}\n",
    "        print(f\"\\n{label.upper()} - Average entity counts (n=200):\")\n",
    "        for etype, count in sorted(avg_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {etype}: {count:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è spaCy model not loaded. Install with: python -m spacy download en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8c61d9",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Lexical & N-gram Patterns\n",
    "- Fully evasive answers have [distinctive unigrams/bigrams compared to direct]\n",
    "- TF-IDF reveals [key terms distinguishing each category]\n",
    "- Vocabulary size [similar/different across categories]\n",
    "\n",
    "### Readability\n",
    "- [Higher/Lower] readability for evasive (easier/harder to read)\n",
    "- Suggests [more complex/simple language patterns]\n",
    "\n",
    "### Sentiment & Hedging\n",
    "- Evasive answers show [more/less] hedging words (XX.X vs Y.Y per answer)\n",
    "- Certainty markers [more/less] frequent in direct answers\n",
    "- Sentiment polarity [biased toward] for certain labels\n",
    "\n",
    "### POS & Entities (if spaCy loaded)\n",
    "- Evasive answers use more/less adjectives/adverbs [check after execution]\n",
    "- Entity density [similar/different] across categories\n",
    "\n",
    "### Linguistic Evasion Markers Identified\n",
    "Top potential evasion markers:\n",
    "1. [List from hedging/certainty analysis]\n",
    "2. [Other features from N-grams]\n",
    "3. [Specific phrases from manual inspection]\n",
    "\n",
    "### Next Steps\n",
    "- Use identified features for Notebook 4 (traditional ML baselines)\n",
    "- Validate linguistic markers with model interpretability (Notebook 9)\n",
    "- Develop rule-based classifier from these markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba675ca",
   "metadata": {},
   "source": [
    "# 02 - Linguistic Patterns\n",
    "\n",
    "**Objectives:**\n",
    "- Lexical and syntactic analysis by label\n",
    "- Readability, POS, NER, hedging analysis\n",
    "- Produce feature candidates for models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
