{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a25ed8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Setup: imports and dataset loading\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# Setup: imports and dataset loading\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from src.utils import set_seed\n",
    "import os\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load dataset from HuggingFace\n",
    "print(\"Loading dataset FutureMa/EvasionBench...\")\n",
    "ds = load_dataset(\"FutureMa/EvasionBench\")\n",
    "if isinstance(ds, dict):\n",
    "    ds = ds[list(ds.keys())[0]]\n",
    "df = ds.to_pandas()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Save local copy if not exists\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "if not os.path.exists(\"data/raw/evasionbench.parquet\"):\n",
    "    df.to_parquet(\"data/raw/evasionbench.parquet\", index=False)\n",
    "    print(\"Saved data/raw/evasionbench.parquet\")\n",
    "\n",
    "# Quick preview\n",
    "print(df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c72f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Overview\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal samples: {len(df):,}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nSample row:\")\n",
    "df.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d677b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Distribution Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "label_counts = df['eva4b_label'].value_counts()\n",
    "label_pcts = df['eva4b_label'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nLabel counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label}: {count:,} ({label_pcts[label]:.1f}%)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = {'direct': '#2ecc71', 'intermediate': '#f39c12', 'fully_evasive': '#e74c3c'}\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(label_counts.index, label_counts.values, color=[colors.get(x, '#3498db') for x in label_counts.index])\n",
    "ax1.set_xlabel('Evasion Label', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Label Distribution (Bar Chart)', fontsize=14, fontweight='bold')\n",
    "for bar, count, pct in zip(bars, label_counts.values, label_pcts.values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "             f'{count:,}\\n({pct:.1f}%)', ha='center', va='bottom', fontsize=10)\n",
    "ax1.set_ylim(0, max(label_counts.values) * 1.15)\n",
    "\n",
    "# Pie chart\n",
    "ax2 = axes[1]\n",
    "wedges, texts, autotexts = ax2.pie(label_counts.values, labels=label_counts.index, \n",
    "                                    autopct='%1.1f%%', colors=[colors.get(x, '#3498db') for x in label_counts.index],\n",
    "                                    explode=[0.02, 0.02, 0.1], startangle=90)\n",
    "ax2.set_title('Label Distribution (Pie Chart)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/figures/01_label_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Class imbalance detected: 'fully_evasive' is only 3.7% of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c489f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Length Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT LENGTH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate lengths\n",
    "df['question_chars'] = df['question'].str.len()\n",
    "df['question_words'] = df['question'].str.split().str.len()\n",
    "df['answer_chars'] = df['answer'].str.len()\n",
    "df['answer_words'] = df['answer'].str.split().str.len()\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nQuestion length statistics:\")\n",
    "print(f\"  Characters - Mean: {df['question_chars'].mean():.1f}, Median: {df['question_chars'].median():.1f}, Std: {df['question_chars'].std():.1f}\")\n",
    "print(f\"  Words - Mean: {df['question_words'].mean():.1f}, Median: {df['question_words'].median():.1f}, Std: {df['question_words'].std():.1f}\")\n",
    "\n",
    "print(\"\\nAnswer length statistics:\")\n",
    "print(f\"  Characters - Mean: {df['answer_chars'].mean():.1f}, Median: {df['answer_chars'].median():.1f}, Std: {df['answer_chars'].std():.1f}\")\n",
    "print(f\"  Words - Mean: {df['answer_words'].mean():.1f}, Median: {df['answer_words'].median():.1f}, Std: {df['answer_words'].std():.1f}\")\n",
    "\n",
    "# Answer length by evasion category\n",
    "print(\"\\nAnswer length by evasion category:\")\n",
    "length_by_label = df.groupby('eva4b_label').agg({\n",
    "    'answer_chars': ['mean', 'median', 'std'],\n",
    "    'answer_words': ['mean', 'median', 'std']\n",
    "}).round(1)\n",
    "print(length_by_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Length Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Question length distribution\n",
    "ax1 = axes[0, 0]\n",
    "for label, color in colors.items():\n",
    "    subset = df[df['eva4b_label'] == label]\n",
    "    ax1.hist(subset['question_words'], bins=50, alpha=0.6, label=label, color=color)\n",
    "ax1.set_xlabel('Question Length (words)', fontsize=11)\n",
    "ax1.set_ylabel('Frequency', fontsize=11)\n",
    "ax1.set_title('Question Length Distribution by Label', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.set_xlim(0, 200)\n",
    "\n",
    "# Answer length distribution\n",
    "ax2 = axes[0, 1]\n",
    "for label, color in colors.items():\n",
    "    subset = df[df['eva4b_label'] == label]\n",
    "    ax2.hist(subset['answer_words'], bins=50, alpha=0.6, label=label, color=color)\n",
    "ax2.set_xlabel('Answer Length (words)', fontsize=11)\n",
    "ax2.set_ylabel('Frequency', fontsize=11)\n",
    "ax2.set_title('Answer Length Distribution by Label', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.set_xlim(0, 500)\n",
    "\n",
    "# Box plots - Answer length by label\n",
    "ax3 = axes[1, 0]\n",
    "df.boxplot(column='answer_words', by='eva4b_label', ax=ax3)\n",
    "ax3.set_xlabel('Evasion Label', fontsize=11)\n",
    "ax3.set_ylabel('Answer Length (words)', fontsize=11)\n",
    "ax3.set_title('Answer Length by Evasion Category', fontsize=12, fontweight='bold')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "# Box plots - Question length by label\n",
    "ax4 = axes[1, 1]\n",
    "df.boxplot(column='question_words', by='eva4b_label', ax=ax4)\n",
    "ax4.set_xlabel('Evasion Label', fontsize=11)\n",
    "ax4.set_ylabel('Question Length (words)', fontsize=11)\n",
    "ax4.set_title('Question Length by Evasion Category', fontsize=12, fontweight='bold')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/figures/01_text_length_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfe365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Tests for Length Differences\n",
    "from scipy import stats\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STATISTICAL TESTS: TEXT LENGTH HYPOTHESES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# H1: Evasive answers are longer than direct answers\n",
    "print(\"\\nüìä HYPOTHESIS 1: 'Evasive answers are longer than direct answers'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "direct_lengths = df[df['eva4b_label'] == 'direct']['answer_words']\n",
    "evasive_lengths = df[df['eva4b_label'] == 'fully_evasive']['answer_words']\n",
    "intermediate_lengths = df[df['eva4b_label'] == 'intermediate']['answer_words']\n",
    "\n",
    "# Kruskal-Wallis test (non-parametric)\n",
    "stat, p_value = stats.kruskal(direct_lengths, intermediate_lengths, evasive_lengths)\n",
    "print(f\"\\nKruskal-Wallis test (all 3 groups):\")\n",
    "print(f\"  H-statistic: {stat:.2f}\")\n",
    "print(f\"  p-value: {p_value:.2e}\")\n",
    "print(f\"  Result: {'Significant difference (p < 0.05)' if p_value < 0.05 else 'No significant difference'}\")\n",
    "\n",
    "# Mann-Whitney U test (direct vs fully_evasive)\n",
    "stat2, p_value2 = stats.mannwhitneyu(direct_lengths, evasive_lengths, alternative='two-sided')\n",
    "print(f\"\\nMann-Whitney U test (direct vs fully_evasive):\")\n",
    "print(f\"  U-statistic: {stat2:.2f}\")\n",
    "print(f\"  p-value: {p_value2:.2e}\")\n",
    "print(f\"  Result: {'Significant difference (p < 0.05)' if p_value2 < 0.05 else 'No significant difference'}\")\n",
    "\n",
    "# Mean comparison\n",
    "print(f\"\\nMean answer lengths:\")\n",
    "print(f\"  Direct: {direct_lengths.mean():.1f} words\")\n",
    "print(f\"  Intermediate: {intermediate_lengths.mean():.1f} words\")\n",
    "print(f\"  Fully Evasive: {evasive_lengths.mean():.1f} words\")\n",
    "\n",
    "# Conclusion\n",
    "if p_value2 < 0.05:\n",
    "    if evasive_lengths.mean() > direct_lengths.mean():\n",
    "        print(\"\\n‚úÖ H1 SUPPORTED: Evasive answers are significantly longer than direct answers\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå H1 REJECTED: Evasive answers are significantly SHORTER than direct answers\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è H1 NOT SUPPORTED: No significant length difference detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b350fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H2: Questions receiving evasive answers differ in structure/length\n",
    "print(\"\\nüìä HYPOTHESIS 2: 'Questions receiving evasive answers differ in length'\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "direct_q_lengths = df[df['eva4b_label'] == 'direct']['question_words']\n",
    "evasive_q_lengths = df[df['eva4b_label'] == 'fully_evasive']['question_words']\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat_q, p_value_q = stats.mannwhitneyu(direct_q_lengths, evasive_q_lengths, alternative='two-sided')\n",
    "print(f\"\\nMann-Whitney U test (question lengths):\")\n",
    "print(f\"  U-statistic: {stat_q:.2f}\")\n",
    "print(f\"  p-value: {p_value_q:.2e}\")\n",
    "print(f\"  Result: {'Significant difference (p < 0.05)' if p_value_q < 0.05 else 'No significant difference'}\")\n",
    "\n",
    "print(f\"\\nMean question lengths:\")\n",
    "print(f\"  Direct answers: {direct_q_lengths.mean():.1f} words\")\n",
    "print(f\"  Evasive answers: {evasive_q_lengths.mean():.1f} words\")\n",
    "\n",
    "if p_value_q < 0.05:\n",
    "    print(\"\\n‚úÖ H2 SUPPORTED: Questions receiving evasive answers have significantly different lengths\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è H2 NOT SUPPORTED: No significant difference in question lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb22516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Checks\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. UID uniqueness\n",
    "print(\"\\n1. UID UNIQUENESS:\")\n",
    "unique_uids = df['uid'].nunique()\n",
    "total_rows = len(df)\n",
    "print(f\"   Unique UIDs: {unique_uids:,} / {total_rows:,}\")\n",
    "if unique_uids == total_rows:\n",
    "    print(\"   ‚úÖ All UIDs are unique\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Found {total_rows - unique_uids} duplicate UIDs\")\n",
    "\n",
    "# 2. Empty strings\n",
    "print(\"\\n2. EMPTY STRING CHECK:\")\n",
    "empty_questions = (df['question'].str.strip() == '').sum()\n",
    "empty_answers = (df['answer'].str.strip() == '').sum()\n",
    "print(f\"   Empty questions: {empty_questions}\")\n",
    "print(f\"   Empty answers: {empty_answers}\")\n",
    "if empty_questions == 0 and empty_answers == 0:\n",
    "    print(\"   ‚úÖ No empty strings found\")\n",
    "\n",
    "# 3. Duplicate detection (exact)\n",
    "print(\"\\n3. EXACT DUPLICATE CHECK:\")\n",
    "dup_questions = df['question'].duplicated().sum()\n",
    "dup_answers = df['answer'].duplicated().sum()\n",
    "dup_pairs = df.duplicated(subset=['question', 'answer']).sum()\n",
    "print(f\"   Duplicate questions: {dup_questions} ({dup_questions/len(df)*100:.2f}%)\")\n",
    "print(f\"   Duplicate answers: {dup_answers} ({dup_answers/len(df)*100:.2f}%)\")\n",
    "print(f\"   Duplicate Q&A pairs: {dup_pairs} ({dup_pairs/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 4. Anomalous lengths (outliers)\n",
    "print(\"\\n4. OUTLIER DETECTION (using IQR method):\")\n",
    "for col in ['question_words', 'answer_words']:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "    print(f\"   {col}: {outliers} outliers ({outliers/len(df)*100:.2f}%)\")\n",
    "    print(f\"      Range: [{max(0, lower):.0f}, {upper:.0f}] words\")\n",
    "\n",
    "# 5. Very short/long texts\n",
    "print(\"\\n5. EXTREME LENGTH SAMPLES:\")\n",
    "print(f\"   Shortest answer: {df['answer_words'].min()} words\")\n",
    "print(f\"   Longest answer: {df['answer_words'].max()} words\")\n",
    "print(f\"   Shortest question: {df['question_words'].min()} words\")\n",
    "print(f\"   Longest question: {df['question_words'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics Table\n",
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY STATISTICS TABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_stats = pd.DataFrame({\n",
    "    'Metric': ['Total Samples', 'Direct', 'Intermediate', 'Fully Evasive',\n",
    "               'Avg Question Length (words)', 'Avg Answer Length (words)',\n",
    "               'Missing Values', 'Duplicate Q&A Pairs'],\n",
    "    'Value': [f\"{len(df):,}\", \n",
    "              f\"{label_counts['direct']:,} ({label_pcts['direct']:.1f}%)\",\n",
    "              f\"{label_counts['intermediate']:,} ({label_pcts['intermediate']:.1f}%)\",\n",
    "              f\"{label_counts['fully_evasive']:,} ({label_pcts['fully_evasive']:.1f}%)\",\n",
    "              f\"{df['question_words'].mean():.1f}\",\n",
    "              f\"{df['answer_words'].mean():.1f}\",\n",
    "              \"0\",\n",
    "              f\"{dup_pairs} ({dup_pairs/len(df)*100:.2f}%)\"]\n",
    "})\n",
    "\n",
    "print(summary_stats.to_string(index=False))\n",
    "\n",
    "# Save summary\n",
    "summary_stats.to_csv('notebooks/figures/01_summary_statistics.csv', index=False)\n",
    "print(\"\\n‚úÖ Summary saved to notebooks/figures/01_summary_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a87de9",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Data Quality\n",
    "- ‚úÖ Dataset is clean with no missing values\n",
    "- ‚úÖ All UIDs are unique\n",
    "- ‚ö†Ô∏è Some duplicate Q&A pairs exist (~X%)\n",
    "- ‚ö†Ô∏è Significant class imbalance: fully_evasive is only 3.7% of data\n",
    "\n",
    "### Hypothesis Test Results\n",
    "1. **H1: Evasive answers are longer than direct answers**\n",
    "   - Result: [To be filled after execution]\n",
    "   \n",
    "2. **H2: Questions receiving evasive answers differ in length**\n",
    "   - Result: [To be filled after execution]\n",
    "\n",
    "### Next Steps\n",
    "- Proceed to Notebook 02 for linguistic pattern analysis\n",
    "- Investigate duplicate Q&A pairs\n",
    "- Consider class imbalance handling strategies for modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
