{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a9200a",
   "metadata": {},
   "source": [
    "# 08 - Multi-Task Learning\n",
    "\n",
    "**Overview:** Implement joint training with auxiliary tasks (sentiment, topic) to improve minority-class robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: multi-task learning skeleton\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "encoder_name = \"roberta-base\"\n",
    "print(\"Loading encoder:\", encoder_name)\n",
    "encoder = AutoModel.from_pretrained(encoder_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "\n",
    "class MultiTaskHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels_primary=3):\n",
    "        super().__init__()\n",
    "        self.shared_pool = nn.Linear(hidden_size, hidden_size)\n",
    "        self.primary = nn.Linear(hidden_size, num_labels_primary)  # evasion\n",
    "        self.aux_sentiment = nn.Linear(hidden_size, 3)  # sentiment\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.shared_pool(x))\n",
    "        return {\n",
    "            \"evasion_logits\": self.primary(h),\n",
    "            \"sentiment_logits\": self.aux_sentiment(h),\n",
    "        }\n",
    "\n",
    "print(\"MTL skeleton ready. Implement dataloaders and training loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1878f",
   "metadata": {},
   "source": [
    "# 08 - Multi-Task Learning\n",
    "\n",
    "**Objectives:**\n",
    "- Joint training experiments with auxiliary tasks\n",
    "- Evaluate impact on minority class (fully_evasive)\n",
    "- Loss weighting and ablation studies"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
