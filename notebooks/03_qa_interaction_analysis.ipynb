{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86209005",
   "metadata": {},
   "source": [
    "# 03 - Q&A Interaction Analysis\n",
    "\n",
    "**Overview:** Analyze question-answer interactions, semantic similarity, question types, and topic patterns to surface conversational signals associated with evasion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1461bb49",
   "metadata": {},
   "source": [
    "# 03 - Q&A Interaction Analysis\n",
    "\n",
    "**Overview:** Analyze interactions between questions and answers to surface conversational patterns and questionâ€“answer dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: topic modeling & embeddings\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from src.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset FutureMa/EvasionBench...\")\n",
    "ds = load_dataset(\"FutureMa/EvasionBench\")\n",
    "if isinstance(ds, dict):\n",
    "    ds = ds[list(ds.keys())[0]]\n",
    "df = ds.to_pandas()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Load lightweight sentence transformer for similarity\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedder loaded: all-MiniLM-L6-v2\")\n",
    "\n",
    "# Example embedding\n",
    "example = df.loc[0, \"question\"] + \" \" + df.loc[0, \"answer\"]\n",
    "print(\"Sample text:\\n\", example[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca76473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Similarity Analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"SEMANTIC SIMILARITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Encode questions and answers\n",
    "print(\"\\nEncoding questions and answers...\")\n",
    "question_embeddings = embedder.encode(df['question'].tolist(), show_progress_bar=True)\n",
    "answer_embeddings = embedder.encode(df['answer'].tolist(), show_progress_bar=True)\n",
    "print(\"Encoding complete.\")\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = [cosine_similarity([q_emb], [a_emb])[0][0] \n",
    "                 for q_emb, a_emb in zip(question_embeddings, answer_embeddings)]\n",
    "\n",
    "df['qa_similarity'] = similarities\n",
    "\n",
    "# Statistics by label\n",
    "print(\"\\nSemantic similarity statistics by evasion category:\")\n",
    "sim_stats = df.groupby('eva4b_label').agg({\n",
    "    'qa_similarity': ['mean', 'std', 'median', 'min', 'max']\n",
    "}).round(3)\n",
    "print(sim_stats)\n",
    "\n",
    "# HYPOTHESIS: Evasive answers have lower semantic similarity to questions\n",
    "print(\"\\nðŸ“Š HYPOTHESIS: 'Evasive answers have lower semantic similarity to questions'\")\n",
    "direct_sim = df[df['eva4b_label'] == 'direct']['qa_similarity'].mean()\n",
    "evasive_sim = df[df['eva4b_label'] == 'fully_evasive']['qa_similarity'].mean()\n",
    "print(f\"  Direct answers mean similarity: {direct_sim:.4f}\")\n",
    "print(f\"  Evasive answers mean similarity: {evasive_sim:.4f}\")\n",
    "print(f\"  Difference: {direct_sim - evasive_sim:.4f}\")\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat, p_value = stats.mannwhitneyu(\n",
    "    df[df['eva4b_label'] == 'direct']['qa_similarity'],\n",
    "    df[df['eva4b_label'] == 'fully_evasive']['qa_similarity'],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "print(f\"  Mann-Whitney U test p-value: {p_value:.2e}\")\n",
    "print(f\"  Result: {'Significant difference (p < 0.05)' if p_value < 0.05 else 'No significant difference'}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    if evasive_sim < direct_sim:\n",
    "        print(\"\\nâœ… HYPOTHESIS SUPPORTED: Evasive answers have significantly lower semantic similarity\")\n",
    "    else:\n",
    "        print(\"\\nâŒ HYPOTHESIS CONTRADICTED: Evasive answers have HIGHER semantic similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e340750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Type Classification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUESTION TYPE CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract question types based on wh-words and patterns\n",
    "question_patterns = {\n",
    "    'what': r'\\bwhat\\b',\n",
    "    'why': r'\\bwhy\\b',\n",
    "    'how': r'\\bhow\\b',\n",
    "    'when': r'\\bwhen\\b',\n",
    "    'where': r'\\bwhere\\b',\n",
    "    'can_you': r'\\bcan you\\b|could you\\b',\n",
    "    'will_you': r'\\bwill you\\b|are you going to\\b',\n",
    "    'do_you': r'\\bdo you\\b|are you\\b',\n",
    "    'is': r'\\bis\\b|are\\b',\n",
    "    'other': r'(?i)(?!.*(what|why|how|when|where|can you|will you|do you|is\\b|are\\b)'\n",
    "}\n",
    "\n",
    "# Classify each question\n",
    "question_type = []\n",
    "import re\n",
    "for q in df['question']:\n",
    "    matched = False\n",
    "    for qtype, pattern in question_patterns.items():\n",
    "        if re.search(pattern, q, re.IGNORECASE):\n",
    "            question_type.append(qtype)\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        question_type.append('other')\n",
    "\n",
    "df['question_type'] = question_type\n",
    "\n",
    "print(\"\\nQuestion type distribution:\")\n",
    "qtype_counts = df['question_type'].value_counts().sort_values(ascending=False)\n",
    "for qtype, count in qtype_counts.items():\n",
    "    print(f\"  {qtype:12s}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Evasion rate by question type\n",
    "print(\"\\nEvasion rate by question type:\")\n",
    "evasion_by_qtype = df.groupby('question_type').agg({\n",
    "    'eva4b_label': lambda x: (x == 'fully_evasive').sum(),\n",
    "    'total': 'size'\n",
    "}).reset_index()\n",
    "evasion_by_qtype['evasion_rate'] = (evasion_by_qtype['eva4b_label'] / evasion_by_qtype['total'] * 100)\n",
    "evasion_by_qtype = evasion_by_qtype.sort_values('evasion_rate', ascending=False)\n",
    "print(evasion_by_qtype[['question_type', 'evasion_rate', 'total']].to_string(index=False))\n",
    "\n",
    "# Types with highest evasion\n",
    "highest_evasion = evasion_by_qtype.iloc[0]['question_type']\n",
    "print(f\"\\nâš ï¸ Question type '{highest_evasion}' elicits the highest evasion rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling on Questions (LDA)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOPIC MODELING - QUESTIONS (LDA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create documents for LDA\n",
    "questions = df['question'].tolist()\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english', min_df=2)\n",
    "doc_term_matrix = vectorizer.fit_transform(questions)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Train LDA model (try different number of topics)\n",
    "n_topics = 15\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "print(f\"\\nTrained LDA with {n_topics} topics on {len(questions)} questions.\")\n",
    "\n",
    "# Display top words for each topic\n",
    "print(\"\\nTop 10 words per topic:\")\n",
    "for topic_idx, topic in enumerate(lda.components_, 1):\n",
    "    top_words_idx = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx}: {', '.join(top_words)}\")\n",
    "\n",
    "# Assign topics to questions\n",
    "question_topics = lda.transform(doc_term_matrix)\n",
    "df['question_topic'] = question_topics.argmax(axis=1)\n",
    "\n",
    "# Topic distribution by evasion category\n",
    "print(\"\\nTopic distribution by evasion category:\")\n",
    "for label in ['direct', 'intermediate', 'fully_evasive']:\n",
    "    subset = df[df['eva4b_label'] == label]\n",
    "    dist = subset['question_topic'].value_counts().sort_index()\n",
    "    print(f\"\\n{label.upper()}:\")\n",
    "    for topic, count in dist.head(5).items():\n",
    "        print(f\"  Topic {topic}: {count:,} ({count/len(subset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbb61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Modeling on Answers & Visualization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOPIC MODELING - ANSWERS (NMF)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use NMF for better topic coherence\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "answers = df['answer'].tolist()\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(answers)\n",
    "\n",
    "# Train NMF\n",
    "n_topics_answer = 15\n",
    "nmf = NMF(n_components=n_topics_answer, random_state=42, max_iter=200)\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"\\nTrained NMF with {n_topics_answer} topics on answers.\")\n",
    "\n",
    "# Display topics\n",
    "print(\"\\nTop words per topic (answers):\")\n",
    "for topic_idx, topic in enumerate(nmf.components_, 1):\n",
    "    top_words_idx = topic.argsort()[-10:][::-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"\\nTopic {topic_idx}: {', '.join(top_words)}\")\n",
    "\n",
    "# Add t-SNE for visualization (optional - can be slow)\n",
    "print(\"\\nâš ï¸ Note: t-SNE visualization is computationally expensive and commented out by default.\")\n",
    "print(\"Uncomment the following cell to generate 2D topic visualization.\")\n",
    "\n",
    "# # Create t-SNE embedding\n",
    "# print(\"Creating t-SNE visualization (this may take several minutes)...\")\n",
    "# answer_topics = nmf.transform(tfidf)\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "# tsne_results = tsne.fit_transform(answer_topics.toarray())\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(12, 10))\n",
    "# colors = {'direct': '#2ecc71', 'intermediate': '#f39c12', 'fully_evasive': '#e74c3c'}\n",
    "# for label, color in colors.items():\n",
    "#     subset = df[df['eva4b_label'] == label]\n",
    "#     plt.scatter(tsne_results[subset.index, 0], tsne_results[subset.index, 1], \n",
    "#                c=color, label=label, alpha=0.6, s=10)\n",
    "# plt.legend()\n",
    "# plt.title('t-SNE of Answer Topics by Evasion Label', fontsize=14, fontweight='bold')\n",
    "# plt.xlabel('t-SNE Dimension 1')\n",
    "# plt.ylabel('t-SNE Dimension 2')\n",
    "# plt.savefig('notebooks/figures/03_tsne_topics.png', dpi=150, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Strategy Taxonomy (Manual Annotation Sample)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAMPLE ANSWER STRATEGY TAXONOMY (50 per category)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create samples for manual coding\n",
    "def strategy_codebook():\n",
    "    \"\"\"Return strategy codebook for manual annotation.\"\"\"\n",
    "    return \"\"\"ANSWER STRATEGY CODEBOOK\n",
    "=====================\n",
    "\n",
    "Answer Types: STRATEGY CODES\n",
    "---\n",
    "1. DIRECT: Question is directly answered with specific information\n",
    "   Examples: \"Our Q4 revenue was $1.2B.\", \"Yes, we expect 5% growth.\"\n",
    "   \n",
    "2. PARTIAL: Some information provided, but question not fully addressed\n",
    "   Examples: \"We saw growth in [specific areas]\", \"That depends on [conditions].\"\n",
    "   \n",
    "3. DEFLECTION: Answer shifts to different topic or question\n",
    "   Examples: \"Let me talk about our strategic priorities.\", \"Important to focus on customer success.\"\n",
    "   \n",
    "4. REFRAMING: Question is paraphrased, answer addresses reformulated question\n",
    "   Examples: \"Regarding growth drivers, we see...\", \"On the topic of profitability...\"\n",
    "   \n",
    "5. VAGUE: Non-specific answer without clear stance or figures\n",
    "   Examples: \"We remain committed to growth.\", \"We're generally optimistic.\"\n",
    "   \n",
    "6. HEDGED: Qualifying language reduces commitment\n",
    "   Examples: \"We expect roughly...\", \"Growth could be around...\", \"It's hard to say exactly.\"\n",
    "   \n",
    "7. REFUSAL: Question is directly avoided or refused\n",
    "   Examples: \"I can't comment on that.\", \"We don't provide guidance.\"\n",
    "\"\"\"\n",
    "\n",
    "print(strategy_codebook())\n",
    "\n",
    "# Sample rows for each category\n",
    "samples_for_annotation = []\n",
    "for label in df['eva4b_label'].unique():\n",
    "    subset = df[df['eva4b_label'] == label].sample(min(50, len(df[df['eva4b_label'] == label]), random_state=42)\n",
    "    samples_for_annotation.append(subset)\n",
    "\n",
    "annotation_sample = pd.concat(samples_for_annotation)\n",
    "\n",
    "# Save annotation sample\n",
    "annotation_path = 'data/annotations/answer_strategy_annotation_sample.csv'\n",
    "annotation_sample[['uid', 'question', 'answer', 'eva4b_label']].to_csv(annotation_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved {len(annotation_sample)} samples to {annotation_path}\")\n",
    "print(\"    - 100 samples (50 per evasion category)\")\n",
    "print(\"    - Add a 'strategy' column with codes from codebook above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bce38",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### Semantic Similarity\n",
    "- Question-answer similarity [calculated] for all pairs\n",
    "- Mean similarity: [to be filled]\n",
    "- Hypothesis result: [To be filled after execution]\n",
    "\n",
    "### Question Types\n",
    "- \"Why\" questions elicit [X% evasion rate]\n",
    "- \"How\" and \"what\" questions [higher/lower] evasion rates\n",
    "- Suggests certain question types are prone to evasion\n",
    "\n",
    "### Topic Modeling\n",
    "- [X] topics identified from questions (LDA)\n",
    "- [Y] topics identified from answers (NMF)\n",
    "- Topics [correlated/didnt correlate] with evasion categories\n",
    "- [Specific topics] tend to receive more evasive answers\n",
    "\n",
    "### Answer Strategies\n",
    "- Sample [50 per category] created for manual annotation\n",
    "- Codebook with 7 strategy types provided\n",
    "- Strategies to identify: DIRECT, PARTIAL, DEFLECTION, REFRAMING, VAGUE, HEDGED, REFUSAL\n",
    "\n",
    "### Insights\n",
    "- Evasive answers may [shift away from question topics]\n",
    "- Question type is [modest/strong] predictor of evasion rate\n",
    "- Semantic similarity [may/may not] be a reliable indicator\n",
    "\n",
    "### Next Steps\n",
    "- Manually annotate answer strategy samples\n",
    "- Use identified patterns for feature engineering (Notebook 4)\n",
    "- Consider question type as input feature for models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e3cb3",
   "metadata": {},
   "source": [
    "# 03 - Q&A Interaction Analysis\n",
    "\n",
    "**Objectives:**\n",
    "- Topic modeling on questions and answers\n",
    "- Semantic similarity and question-type analysis\n",
    "- Build answer strategy taxonomy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
