---
phase: 05-classical-baseline-modeling
plan: "04"
type: execute
wave: 3
depends_on:
  - "02"
  - "03"
files_modified:
  - scripts/compare_classical_models.py
  - scripts/run_classical_baselines.py
  - src/visualization.py
  - docs/analysis_workflow.md
  - tests/test_phase5_model_comparison_artifacts.py
autonomous: true
user_setup: []
must_haves:
  truths:
    - Classical model families are compared in a single deterministic summary artifact for report/dashboard ingestion.
    - Comparison outputs include per-class deltas and confusion-matrix references for every model run.
    - Phase-5 outputs are fully consumable by downstream reporting and serving layers.
  artifacts:
    - scripts/compare_classical_models.py
    - tests/test_phase5_model_comparison_artifacts.py
    - artifacts/models/phase5/model_comparison/*
    - docs/analysis_workflow.md
  key_links:
    - compare_classical_models.py reads contract outputs from each family and writes a consolidated ranking + per-class comparison table.
    - run_classical_baselines.py can optionally trigger the comparison step after family runs complete.
    - docs document artifact locations consumed by report and dashboard phases.
---

<objective>
Create cross-family comparison outputs and reporting hooks for Phase 5.

Purpose: close Phase 5 success criteria by making classical baseline results directly consumable by report/dashboard layers.
Output: model-comparison script, charts/tables, and tests validating artifact completeness.
</objective>

<execution_context>
@/Users/dustinober/.codex/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.codex/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-classical-baseline-modeling/05-RESEARCH.md
@.planning/phases/05-classical-baseline-modeling/05-02-SUMMARY.md
@.planning/phases/05-classical-baseline-modeling/05-03-SUMMARY.md
@src/evaluation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build deterministic model-comparison aggregator</name>
  <files>scripts/compare_classical_models.py, scripts/run_classical_baselines.py</files>
  <action>Implement a comparator that ingests family artifact contracts and writes consolidated tables (`model_ranking.csv`, `per_class_f1_comparison.csv`) plus machine-readable summary JSON with metric deltas and artifact pointers.</action>
  <verify>`python scripts/compare_classical_models.py --input-root artifacts/models/phase5 --output-root artifacts/models/phase5/model_comparison` writes expected outputs.</verify>
  <done>Classical families are ranked and comparable in one canonical artifact set.</done>
</task>

<task type="auto">
  <name>Task 2: Add comparison visualization outputs and docs for downstream consumers</name>
  <files>src/visualization.py, docs/analysis_workflow.md</files>
  <action>Generate chart artifacts (for example macro-F1 bar chart and per-class delta heatmap) and document exact file paths/fields consumed by reporting/dashboard layers.</action>
  <verify>`rg -n "phase5|model_comparison|per_class_f1" docs/analysis_workflow.md` confirms documented handoff fields and commands.</verify>
  <done>Phase-5 results are immediately reusable by Phase 7 reporting and Phase 8 UI/API views.</done>
</task>

<task type="auto">
  <name>Task 3: Add comparison artifact-contract tests and smoke command</name>
  <files>tests/test_phase5_model_comparison_artifacts.py, scripts/run_classical_baselines.py</files>
  <action>Add tests validating required comparison files/keys and data consistency between ranking table and source family metrics. Add a smoke command path to run all families + comparison in one call.</action>
  <verify>`pytest -q tests/test_phase5_model_comparison_artifacts.py` and `python scripts/run_classical_baselines.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/models/phase5 --families all --compare` pass.</verify>
  <done>Phase-5 output contract is complete and ready for downstream pipeline integration.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python scripts/run_classical_baselines.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/models/phase5 --families all --compare`
- [ ] `python scripts/compare_classical_models.py --input-root artifacts/models/phase5 --output-root artifacts/models/phase5/model_comparison`
- [ ] `pytest -q tests/test_phase5_model_comparison_artifacts.py`
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Phase-5 outputs are report/dashboard-consumable with traceable model-comparison artifacts
</success_criteria>

<output>
After completion, create `.planning/phases/05-classical-baseline-modeling/05-04-SUMMARY.md`
</output>
