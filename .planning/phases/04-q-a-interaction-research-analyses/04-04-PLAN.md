---
phase: 04-q-a-interaction-research-analyses
plan: "04"
type: execute
wave: 2
depends_on:
  - "01"
files_modified:
  - src/analysis/question_behavior.py
  - scripts/analyze_question_behavior.py
  - tests/test_question_behavior_analysis.py
  - docs/analysis_workflow.md
autonomous: true
user_setup: []
must_haves:
  truths:
    - Question-type and behavior analyses run reproducibly and produce label-stratified outputs tied to evasion hypotheses.
    - A stable question-taxonomy schema is persisted with comparative metrics and report-ready visualizations.
    - Outputs integrate into shared phase-4 artifact indexing for downstream report builds.
  artifacts:
    - src/analysis/question_behavior.py
    - scripts/analyze_question_behavior.py
    - tests/test_question_behavior_analysis.py
    - artifacts/analysis/phase4/question_behavior/*
  key_links:
    - scripts/analyze_question_behavior.py invokes question-type extraction and behavior metrics in src/analysis/question_behavior.py.
    - outputs include taxonomy tables, comparative behavior metrics, and interpretation summaries by label.
    - docs/analysis_workflow.md documents taxonomy assumptions and reproducibility controls.
---

<objective>
Implement question-type and question-behavior analyses connected to evasion hypotheses.

Purpose: Complete `ANLY-06` by producing reproducible question-behavior evidence that complements semantic/topic findings.
Output: Question-behavior analysis module + CLI script + tests + documented taxonomy and artifact outputs.
</objective>

<execution_context>
@/Users/dustinober/.codex/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.codex/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-q-a-interaction-research-analyses/04-RESEARCH.md
@.planning/phases/04-q-a-interaction-research-analyses/04-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define deterministic question taxonomy and extraction pipeline</name>
  <files>src/analysis/question_behavior.py, scripts/analyze_question_behavior.py</files>
  <action>Implement a stable question-type taxonomy (for example factual, opinion, procedural, comparison, yes/no, multi-part) and deterministic extraction/assignment logic from question text. Persist row-level assignments and taxonomy metadata.</action>
  <verify>`python scripts/analyze_question_behavior.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/analysis/phase4 --emit-assignments` writes taxonomy assignment artifacts and metadata.</verify>
  <done>Question-type labels are reproducible and traceable for behavior analysis.</done>
</task>

<task type="auto">
  <name>Task 2: Add label-stratified behavior metrics and hypothesis interpretation outputs</name>
  <files>src/analysis/question_behavior.py, scripts/analyze_question_behavior.py</files>
  <action>Compute behavior metrics by question type and evasiveness label (for example answer length shifts, semantic alignment deltas, refusal/evasive marker rates) and export comparative charts/tables with concise hypothesis-linked summaries.</action>
  <verify>`python scripts/analyze_question_behavior.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/analysis/phase4 --emit-summary` produces behavior comparison outputs and interpretation artifacts.</verify>
  <done>Question-behavior findings are report-ready and explicitly linked to evasion hypotheses.</done>
</task>

<task type="auto">
  <name>Task 3: Add tests for taxonomy contract and behavior-metric schemas</name>
  <files>tests/test_question_behavior_analysis.py</files>
  <action>Create tests that validate required taxonomy categories, output schema contracts, deterministic behavior on fixtures, and failure behavior for missing question/label fields.</action>
  <verify>`pytest -q tests/test_question_behavior_analysis.py` passes with schema and negative-case validations.</verify>
  <done>Question-behavior analysis is stable and safe for automated pipeline use.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python scripts/analyze_question_behavior.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/analysis/phase4`
- [ ] `pytest -q tests/test_question_behavior_analysis.py`
- [ ] `python -m json.tool artifacts/analysis/phase4/question_behavior/question_behavior_summary.json >/dev/null`
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- ANLY-06 question-behavior outputs are reproducible and hypothesis-oriented
</success_criteria>

<output>
After completion, create `.planning/phases/04-q-a-interaction-research-analyses/04-04-SUMMARY.md`
</output>
