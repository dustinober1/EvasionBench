---
phase: 06-transformer-explainability-label-diagnostics
plan: "04"
type: execute
wave: 4
depends_on: ["05-classical-baseline-modeling"]
files_modified:
  - src/models.py
  - scripts/run_label_diagnostics.py
  - dvc.yaml
  - Makefile
  - tests/test_label_diagnostics.py
  - docs/label_quality_guide.md
autonomous: true
user_setup: []
must_haves:
  truths:
    - Cleanlab label quality diagnostics identify potential label errors and outliers.
    - Diagnostics run on training data only (no test contamination).
    - Outputs include suspect examples with issue types and confidence scores.
    - Findings are exportable as CSV for review and JSON for pipeline consumption.
  artifacts:
    - src/models.py (label quality helper functions)
    - scripts/run_label_diagnostics.py (Cleanlab orchestration)
    - dvc.yaml (phase6_label_diagnostics stage)
    - Makefile (label-diagnostics target)
    - tests/test_label_diagnostics.py
    - docs/label_quality_guide.md
  key_links:
    - scripts/run_label_diagnostics.py runs Cleanlab Datalab on training features/labels.
    - src/models.py exposes compute_pred_probs_for_diagnostics() using TF-IDF + LogisticRegression.
    - Artifacts include suspect examples CSV with issue types (label error, outlier, near_duplicate).
    - Documentation explains issue types and how to interpret diagnostic confidence scores.
---

<objective>
Run label quality diagnostics using Cleanlab to identify noise and ambiguity.

Purpose: detect potential label errors, outliers, and near-duplicates in training data to support XAI-03 requirement and inform future data quality iterations.
Output: suspect examples with issue types, diagnostic summary statistics, and actionable recommendations.
</objective>

<execution_context>
@/home/dusitnober/.claude/get-shit-done/workflows/execute-plan.md
@/home/dusitnober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-transformer-explainability-label-diagnostics/06-RESEARCH.md
@.planning/phases/05-classical-baseline-modeling/05-01-SUMMARY.md
@src/models.py
@src/evaluation.py
@scripts/run_classical_baselines.py
@data/processed/evasionbench_prepared.parquet
@dvc.yaml
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Cleanlab-based label quality diagnostics helper</name>
  <files>src/models.py</files>
  <action>Add to src/models.py:

compute_pred_probs_for_diagnostics() function that:
- Takes prepared DataFrame and random state as inputs
- Uses same feature building as Phase 5: question + " [SEP] " + answer
- Creates TF-IDF vectorizer with max_features=5000 (consistent with logreg baseline)
- Fits LogisticRegression with balanced class weight
- Returns predict_proba output, feature matrix, fitted vectorizer, and model

run_label_diagnostics() function that:
- Takes DataFrame, output directory, and optional random state
- Splits data using SAME split parameters as Phase 5 (random_state=42, test_size=0.2)
- Runs diagnostics on TRAINING split only (critical: no test leakage)
- Calls compute_pred_probs_for_diagnostics() to get features and probabilities
- Initializes Cleanlab Datalab with label_name="label_int"
- Calls lab.find_issues(pred_probs=pred_probs, features=X.toarray())
- Extracts issue types: label, outlier, near_duplicate, class_imbalance, non_iid
- Returns Datalab object and issue summary dataframe

DO NOT run on test data. DO NOT modify original labels (diagnostics only).
</action>
  <verify>`pytest -q tests/test_label_diagnostics.py -k "test_diagnostics_run"` validates Cleanlab execution.</verify>
  <done>Label quality diagnostics run reproducibly on training data with Cleanlab.</done>
</task>

<task type="auto">
  <name>Task 2: Create label diagnostics orchestration script with comprehensive outputs</name>
  <files>scripts/run_label_diagnostics.py</files>
  <action>Create scripts/run_label_diagnostics.py that:
- Accepts --input (prepared data path), --output-root, --random-state (default: 42)
- Loads prepared data and maps labels to integers: {"non_evasive": 0, "evasive": 1}
- Calls run_label_diagnostics() from src.models
- Generates outputs:
  * suspect_examples.csv: rows with is_label_issue=True, including text, label, label_score, issue types
  * label_diagnostics_summary.json: issue counts, quality score, problematic indices
  * label_diagnostics_report.md: human-readable interpretation with examples
  * outlier_examples.csv: rows flagged as outliers with scores
  * near_duplicate_pairs.csv: detected near-duplicate pairs with similarity scores
- Logs diagnostic run to MLflow with issue counts as metrics
- Writes actionable recommendations (e.g., "X suspect examples: recommend manual review")

DO NOT modify original data file. DO clearly separate training vs test data in diagnostics.</action>
  <verify>`python scripts/run_label_diagnostics.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/diagnostics/phase6` generates all diagnostic artifacts.</verify>
  <done>Users can identify label quality issues via one script command.</done>
</task>

<task type="auto">
  <name>Task 3: Wire DVC stage, add Make target, write tests, and document interpretation</name>
  <files>dvc.yaml, Makefile, tests/test_label_diagnostics.py, docs/label_quality_guide.md</files>
  <action>Add to dvc.yaml:
- phase6_label_diagnostics stage invoking run_label_diagnostics.py
- Depends on data/processed/evasionbench_prepared.parquet (trained on the fly, no model dep)
- Outputs to artifacts/diagnostics/phase6 with CSV/JSON/MD artifacts

Add to Makefile:
- label-diagnostics target calling script with canonical args

Create tests/test_label_diagnostics.py:
- test_diagnostics_schema: validates output files exist with required keys
- test_diagnostics_training_only: confirms test data not included in diagnostics
- test_diagnostics_reproducibility: verifies same random state produces identical results
- test_diagnostics_issue_types: checks expected issue types are detected
- test_detrics_csv_format: validates CSV structure for downstream review

Create docs/label_quality_guide.md:
- Explain Cleanlab approach: confident learning, cross-validation, probabilistic label modeling
- Document each issue type:
  * label_error: examples where predicted label disagrees with given label
  * outlier: examples far from distribution in feature space
  * near_duplicate: highly similar example pairs (potential labeling inconsistency)
  * class_imbalance: skewed class distribution affecting model learning
- Explain label_score: probability that given label is correct (low = suspect)
- Show how to review suspect_examples.csv and criteria for manual review
- Note limitations: diagnostics depend on feature quality, false positives possible
- Provide workflow: 1) run diagnostics, 2) review suspects, 3) correct labels if needed, 4) re-train</action>
  <verify>`pytest -q tests/test_label_diagnostics.py` passes all diagnostic tests.</verify>
  <done>Label diagnostics are integrated, tested, and documented with Clear interpretation guide.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest -q tests/test_label_diagnostics.py`
- [ ] `python scripts/run_label_diagnostics.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/diagnostics/phase6`
- [ ] Artifacts exist: suspect_examples.csv, label_diagnostics_summary.json, label_diagnostics_report.md
- [ ] docs/label_quality_guide.md explains all issue types and interpretation
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Cleanlab diagnostics identify label issues without modifying data
- Outputs support both human review (CSV/MD) and pipeline consumption (JSON)
- Documentation enables informed decisions about label corrections
</success_criteria>

<output>
After completion, create `.planning/phases/06-transformer-explainability-label-diagnostics/06-04-SUMMARY.md`
</output>
