---
phase: 06-transformer-explainability-label-diagnostics
plan: "03"
type: execute
wave: 3
depends_on: ["06-01"]
files_modified:
  - src/explainability.py
  - scripts/run_transformer_explainability.py
  - dvc.yaml
  - Makefile
  - tests/test_transformer_explainability.py
  - docs/transformer_xai_guide.md
autonomous: true
user_setup: []
must_haves:
  truths:
    - Captum/LIME explainability artifacts are generated for transformer predictions.
    - Artifacts include word-level attributions for representative samples with visual explanations.
    - Output format is JSON for machine data and HTML for interactive inspection.
    - Explanations work on loaded transformer checkpoints from Phase 6 plan 01.
  artifacts:
    - src/explainability.py (transformer XAI functions)
    - scripts/run_transformer_explainability.py (orchestration)
    - dvc.yaml (phase6_xai_transformer stage)
    - Makefile (xai-transformer target)
    - tests/test_transformer_explainability.py
    - docs/transformer_xai_guide.md
  key_links:
    - scripts/run_transformer_explainability.py loads transformer model and generates Captum attributions.
    - src/explainability.py exposes explain_transformer_instance() using LayerIntegratedGradients.
    - Artifacts include token-level attributions aligned with original text for interpretability.
    - Documentation explains gradient-based attribution and how to interpret word importance.
---

<objective>
Generate explainability artifacts for transformer predictions using Captum.

Purpose: provide word-level attributions for DistilBERT predictions to support XAI-02 requirement and complement classical SHAP explanations.
Output: token-level importance scores, visual explanations, and representative sample analysis.
</objective>

<execution_context>
@/home/dusitnober/.claude/get-shit-done/workflows/execute-plan.md
@/home/dusitnober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-transformer-explainability-label-diagnostics/06-RESEARCH.md
@.planning/phases/06-transformer-explainability-label-diagnostics/06-01-SUMMARY.md
@src/explainability.py
@src/models.py
@scripts/run_transformer_baselines.py
@dvc.yaml
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Captum-based transformer explainability</name>
  <files>src/explainability.py</files>
  <action>Add to src/explainability.py:

explain_transformer_instance() function that:
- Takes loaded transformer model, tokenizer, text sample, and true label as inputs
- Uses Captum LayerIntegratedGradients with model.distilbert.embeddings as target layer
- Tokenizes input text and generates attributions for the predicted class
- Aligns attributions with tokens (handling [CLS], [SEP], special tokens)
- Returns:
  * tokens: list of token strings
  * attributions: numpy array of attribution scores
  * predicted_label: string (evasive/non_evasive)
  * attribution_sum: sum of attributions (sanity check)

explain_transformer_batch() function that:
- Takes model, tokenizer, data samples, and output directory
- Selects representative samples (5 correct predictions, 5 mispredictions per class if available)
- Generates attributions for each sample using explain_transformer_instance()
- Outputs transformer_xai.json with all sample explanations
- Creates HTML visualization showing highlighted text by attribution strength

DO NOT use attention visualization (research shows poor correlation with true importance).
DO handle [CLS]/[SEP] tokens by removing or marking them in output.
</action>
  <verify>`pytest -q tests/test_transformer_explainability.py -k "test_captum_single"` validates single-sample attribution.</verify>
  <done>Transformer predictions have word-level Captum attributions for interpretability.</done>
</task>

<task type="auto">
  <name>Task 2: Create transformer explainability orchestration script</name>
  <files>scripts/run_transformer_explainability.py</files>
  <action>Create scripts/run_transformer_explainability.py that:
- Loads trained transformer from artifacts/models/phase6/transformer/
- Loads prepared data from data/processed/evasionbench_prepared.parquet
- Accepts --model-path, --data-path, --output-root, --n-samples (default: 20)
- Selects representative samples:
  * Balance across true labels (evasive/non_evasive)
  * Include both correct and incorrect predictions if available
  * Use stratified sampling to cover prediction types
- Calls explain_transformer_batch() to generate attributions
- Outputs:
  * transformer_xai.json: sample-level attributions with text, tokens, scores, labels
  * transformer_xai_summary.json: aggregate stats (n_samples, avg_attribution_sum, top_tokens)
  * transformer_xai.html: interactive visualization with highlighted text
- Logs run to MLflow with explainer type and sample count metadata

DO ensure model is in eval mode during attribution. DO use torch.no_grad() where appropriate.</action>
  <verify>`python scripts/run_transformer_explainability.py --model-path artifacts/models/phase6/transformer --output-root artifacts/explainability/phase6/transformer --n-samples 10` generates XAI artifacts.</verify>
  <done>Users can generate transformer XAI artifacts via one script command.</done>
</task>

<task type="auto">
  <name>Task 3: Wire DVC stage, add Make target, write tests, and document</name>
  <files>dvc.yaml, Makefile, tests/test_transformer_explainability.py, docs/transformer_xai_guide.md</files>
  <action>Add to dvc.yaml:
- phase6_xai_transformer stage invoking run_transformer_explainability.py
- Depends on phase6_transformer model outputs
- Outputs to artifacts/explainability/phase6/transformer

Add to Makefile:
- xai-transformer target calling script with canonical args
- xai-all target running both classical and transformer XAI

Create tests/test_transformer_explainability.py:
- test_captum_single_sample: validates explain_transformer_instance() returns tokens and attributions
- test_captum_batch_output: checks transformer_xai.json exists with required keys
- test_captum_html_output: verifies transformer_xai.html is generated
- test_captum_reproducibility: ensures fixed samples produce same attributions
- test_captum_token_alignment: verifies attribution array length matches token count

Create docs/transformer_xai_guide.md:
- Explain Captum LayerIntegratedGradients: integrates gradients along path from baseline to input
- Interpretation: positive attribution = pushes toward prediction, negative = pushes away
- Show how to read transformer_xai.json structure
- Include example of loading and visualizing attributions
- Note differences from SHAP: gradient-based vs game-theoretic, local-only vs global+local
- Document limitations: computational cost, sensitivity to baseline choice</action>
  <verify>`pytest -q tests/test_transformer_explainability.py` passes all transformer XAI tests.</verify>
  <done>Transformer XAI is integrated, tested, and documented with Captum.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest -q tests/test_transformer_explainability.py`
- [ ] `python scripts/run_transformer_explainability.py --model-path artifacts/models/phase6/transformer --output-root artifacts/explainability/phase6/transformer --n-samples 10`
- [ ] Artifacts exist: transformer_xai.json, transformer_xai.html
- [ ] docs/transformer_xai_guide.md explains Captum attribution interpretation
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Captum attributions generated for representative samples
- HTML visualization provides interpretable word highlighting
- Documentation contrasts transformer vs classical XAI approaches
</success_criteria>

<output>
After completion, create `.planning/phases/06-transformer-explainability-label-diagnostics/06-03-SUMMARY.md`
</output>
