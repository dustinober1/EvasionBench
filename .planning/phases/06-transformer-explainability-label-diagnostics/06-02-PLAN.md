---
phase: 06-transformer-explainability-label-diagnostics
plan: "02"
type: execute
wave: 2
depends_on: ["05-classical-baseline-modeling", "06-01"]
files_modified:
  - src/explainability.py
  - scripts/run_explainability_analysis.py
  - dvc.yaml
  - Makefile
  - tests/test_explainability_artifacts.py
  - docs/explainability_guide.md
autonomous: true
user_setup: []
must_haves:
  truths:
    - SHAP explainability artifacts are generated for all Phase 5 classical models.
    - Artifacts include global feature importance (summary plot data) and local explanations (per-sample attributions).
    - Output format is JSON for machine-readable consumption and PNG for report figures.
    - All XAI runs respect train/test split boundaries (no leakage).
  artifacts:
    - src/explainability.py (SHAP implementation)
    - scripts/run_explainability_analysis.py (orchestration)
    - dvc.yaml (phase6_xai_classical stage)
    - Makefile (xai-classical target)
    - tests/test_explainability_artifacts.py
    - docs/explainability_guide.md
  key_links:
    - scripts/run_explainability_analysis.py loads phase-5 models and generates SHAP values.
    - src/explainability.py exposes explain_classical_model() with LinearExplainer for logreg and TreeExplainer for tree/boosting.
    - Artifacts include shap_summary.json (global importance) and shap_samples.json (local explanations).
    - Documentation explains SHAP values, limitations, and usage for report generation.
---

<objective>
Generate explainability artifacts for classical baseline models using SHAP.

Purpose: produce feature importance and local explanations for Phase 5 models (logreg, tree, boosting) to support XAI-01 requirement and downstream reporting.
Output: SHAP values for global feature importance, per-sample local explanations, and visualization-ready artifacts.
</objective>

<execution_context>
@/home/dusitnober/.claude/get-shit-done/workflows/execute-plan.md
@/home/dusitnober/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-transformer-explainability-label-diagnostics/06-RESEARCH.md
@.planning/phases/05-classical-baseline-modeling/05-01-SUMMARY.md
@src/explainability.py
@src/models.py
@src/evaluation.py
@scripts/run_classical_baselines.py
@dvc.yaml
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement SHAP explainability for classical model families</name>
  <files>src/explainability.py</files>
  <action>Replace placeholder in src/explainability.py with:

explain_classical_model() function that:
- Takes model artifact directory, data path, and output directory as inputs
- Loads model from phase-5 artifacts (supports Pipeline for logreg, fitted estimator for tree/boosting)
- Detects model family and selects appropriate explainer:
  * LinearExplainer for TF-IDF + LogisticRegression
  * TreeExplainer for RandomForest and HistGradientBoosting
- Computes SHAP values on training data (NOT test data - avoid leakage)
- Generates global feature importance: top 20 features by mean absolute SHAP value
- Generates local explanations: SHAP values for 10 representative samples (5 per class)
- Returns feature names, shap_values array, and sample indices

DO NOT compute SHAP on test data. DO NOT use KernelExplainer (too slow for text models).

Output artifacts:
- shap_summary.json: {"feature_names": [...], "importance_ranking": [...], "mean_abs_shap": [...]}
- shap_samples.json: {"indices": [...], "shap_values": [...], "true_labels": [...], "predicted_labels": [...]}
- shap_summary.png: matplotlib summary plot saved as figure
</action>
  <verify>`pytest -q tests/test_explainability_artifacts.py -k "test_shap_classical"` validates SHAP output schemas.</verify>
  <done>All Phase 5 classical models have SHAP-based explainability artifacts.</done>
</task>

<task type="auto">
  <name>Task 2: Create explainability orchestration script with model family selection</name>
  <files>scripts/run_explainability_analysis.py</files>
  <action>Create scripts/run_explainability_analysis.py that:
- Mirrors run_classical_baselines.py CLI structure
- Supports --families {all,logreg,tree,boosting} for selective XAI generation
- Accepts --models-root pointing to phase-5 artifacts (default: artifacts/models/phase5)
- Accepts --output-root for XAI artifacts (default: artifacts/explainability/phase6)
- For each family:
  * Loads model from models-root/{family}/
  * Calls explainability.explain_classical_model()
  * Writes summary and sample artifacts to output-root/{family}/
  * Logs run metadata (model family, n_samples, n_features, explainer type) to MLflow if enabled
- Generates combined summary across all families in output-root/xai_summary.json

DO NOT compute on test data. DO ensure train-only data usage for SHAP computation.</action>
  <verify>`python scripts/run_explainability_analysis.py --models-root artifacts/models/phase5 --output-root artifacts/explainability/phase6 --families logreg` creates SHAP artifacts for logreg.</verify>
  <done>Users can generate XAI artifacts for any classical model family via one command.</done>
</task>

<task type="auto">
  <name>Task 3: Wire DVC stage, add Make target, write tests, and document usage</name>
  <files>dvc.yaml, Makefile, tests/test_explainability_artifacts.py, docs/explainability_guide.md</files>
  <action>Add to dvc.yaml:
- phase6_xai_classical stage invoking run_explainability_analysis.py with families=all
- Depends on phase5 outputs (logreg/tree/boosting artifact directories)
- Outputs to artifacts/explainability/phase6 with per-family subdirectories

Add to Makefile:
- xai-classical target calling script with canonical args

Create tests/test_explainability_artifacts.py:
- test_shap_schema_exists: validates shap_summary.json and shap_samples.json exist
- test_shap_summary_keys: checks required keys (feature_names, importance_ranking, mean_abs_shap)
- test_shap_samples_keys: checks required keys (indices, shap_values, labels)
- test_shap_no_test_leakage: ensures SHAP computed on training data only
- test_shap_reproducibility: verifies deterministic output with fixed random state

Create docs/explainability_guide.md:
- Explain SHAP values: positive = pushes toward evasive, negative = pushes toward non-evasive
- Document per-model explainer types and their interpretations
- Show how to read shap_summary.json and shap_samples.json
- Include example code for loading and plotting SHAP values
- Note limitations: training data only, not post-hoc on test predictions</action>
  <verify>`pytest -q tests/test_explainability_artifacts.py` passes all XAI contract tests.</verify>
  <done>Classical XAI is integrated, tested, and documented for report generation.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `pytest -q tests/test_explainability_artifacts.py`
- [ ] `python scripts/run_explainability_analysis.py --models-root artifacts/models/phase5 --output-root artifacts/explainability/phase6 --families all`
- [ ] Artifacts exist: artifacts/explainability/phase6/{logreg,tree,boosting}/shap_*.json
- [ ] docs/explainability_guide.md documents SHAP interpretation
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- SHAP artifacts generated for all three classical families
- No test data leakage in SHAP computation
- Documentation explains SHAP values for report authors
</success_criteria>

<output>
After completion, create `.planning/phases/06-transformer-explainability-label-diagnostics/06-02-SUMMARY.md`
</output>
