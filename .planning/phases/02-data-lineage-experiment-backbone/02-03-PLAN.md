---
phase: 02-data-lineage-experiment-backbone
plan: "03"
type: execute
wave: 2
depends_on:
  - "01"
files_modified:
  - scripts/run_experiment.py
  - src/models.py
  - src/evaluation.py
  - tests/test_mlflow_tracking.py
  - docs/mlflow_guide.md
autonomous: true
user_setup: []
must_haves:
  truths:
    - Training/evaluation runs are executable from a script entrypoint and log params/metrics to MLflow.
    - Each MLflow run captures reproducibility metadata (dataset checksum/revision and code/run identifiers).
    - Experiment outputs can be inspected consistently in local MLflow UI without notebook-only steps.
  artifacts:
    - scripts/run_experiment.py
    - src/models.py
    - src/evaluation.py
    - tests/test_mlflow_tracking.py
    - docs/mlflow_guide.md
  key_links:
    - scripts/run_experiment.py starts and closes MLflow runs around model train/eval lifecycle.
    - run_experiment logs dataset provenance tags sourced from the data manifest.
    - tests/test_mlflow_tracking.py asserts params, metrics, and tags exist in produced MLflow runs.
---

<objective>
Add script-first MLflow experiment tracking with reproducibility metadata.

Purpose: Satisfy `DATA-04` and make experiment history auditable for later modeling/reporting phases.
Output: Experiment runner script, MLflow-aware model/evaluation hooks, tests, and updated MLflow guide.
</objective>

<execution_context>
@/Users/dustinober/.codex/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.codex/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-lineage-experiment-backbone/02-RESEARCH.md
@.planning/phases/02-data-lineage-experiment-backbone/02-01-SUMMARY.md
@src/models.py
@src/evaluation.py
@docs/mlflow_guide.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build script-first experiment runner with MLflow logging</name>
  <files>scripts/run_experiment.py</files>
  <action>Create a CLI experiment runner that loads prepared data, trains a baseline model, evaluates it, and logs params/metrics/artifacts to MLflow. Add run tags for dataset checksum, dataset revision, git SHA (if available), and pipeline stage identifiers.</action>
  <verify>`python scripts/run_experiment.py --tracking-uri file:./mlruns --experiment-name evasionbench-baselines` completes and records a run in local MLflow store.</verify>
  <done>Experiments run outside notebooks and produce MLflow-tracked runs with provenance metadata.</done>
</task>

<task type="auto">
  <name>Task 2: Wire model/evaluation modules for structured metric emission</name>
  <files>src/models.py, src/evaluation.py</files>
  <action>Implement minimal baseline training/evaluation helpers returning structured metric dictionaries and artifact paths that can be logged directly by `run_experiment.py`. Keep API deterministic and compatible with script execution.</action>
  <verify>`python -c "from src.evaluation import *"` imports cleanly and integration path from training through metrics logging succeeds in experiment script execution.</verify>
  <done>Model/evaluation code exposes metrics in a consistent shape consumable by MLflow logging.
</done>
</task>

<task type="auto">
  <name>Task 3: Add MLflow tracking tests and operational runbook updates</name>
  <files>tests/test_mlflow_tracking.py, docs/mlflow_guide.md</files>
  <action>Add tests that run a lightweight local experiment and assert required params/metrics/tags are recorded. Update MLflow guide with canonical run command, tracking URI options, and troubleshooting notes for missing runs/artifacts.</action>
  <verify>`pytest -q tests/test_mlflow_tracking.py` passes and `docs/mlflow_guide.md` reflects real command + metadata fields from the implementation.</verify>
  <done>MLflow integration is test-backed and usable by contributors from documented commands.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python scripts/run_experiment.py --tracking-uri file:./mlruns --experiment-name evasionbench-baselines`
- [ ] `pytest -q tests/test_mlflow_tracking.py`
- [ ] `mlflow ui --backend-store-uri ./mlruns --port 5000` shows run with params, metrics, and provenance tags
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Experiment runs are script-first, tracked, and reproducible in MLflow
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-lineage-experiment-backbone/02-03-SUMMARY.md`
</output>
