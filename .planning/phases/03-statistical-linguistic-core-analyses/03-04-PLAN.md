---
phase: 03-statistical-linguistic-core-analyses
plan: "04"
type: execute
wave: 2
depends_on:
  - "01"
files_modified:
  - src/analysis/linguistic_quality.py
  - scripts/analyze_linguistic_quality.py
  - tests/test_linguistic_quality_analysis.py
  - docs/analysis_workflow.md
autonomous: true
user_setup:
  - python -m spacy download en_core_web_sm
must_haves:
  truths:
    - Readability, POS, and discourse marker analyses are generated by label with consistent table/chart formatting.
    - Linguistic-quality outputs include both aggregate metrics and interpretation-ready summaries.
    - Generated artifacts are persisted within the shared phase-3 contract for downstream report steps.
  artifacts:
    - src/analysis/linguistic_quality.py
    - scripts/analyze_linguistic_quality.py
    - tests/test_linguistic_quality_analysis.py
    - artifacts/analysis/phase3/linguistic_quality/*
  key_links:
    - scripts/analyze_linguistic_quality.py invokes textstat/spacy-based analysis with deterministic processing parameters.
    - linguistic-quality analysis emits readability, POS, and discourse marker tables plus chart outputs in stable paths.
    - docs/analysis_workflow.md includes model setup and command verification guidance.
---

<objective>
Implement readability, POS, and discourse-marker analysis outputs by evasiveness label.

Purpose: Satisfy `ANLY-04` with script-first linguistic-quality artifacts and standardized formatting.
Output: Linguistic quality module + CLI script + tests producing reproducible tables/charts/summary artifacts.
</objective>

<execution_context>
@/Users/dustinober/.codex/get-shit-done/workflows/execute-plan.md
@/Users/dustinober/.codex/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-statistical-linguistic-core-analyses/03-RESEARCH.md
@.planning/phases/03-statistical-linguistic-core-analyses/03-01-SUMMARY.md
@notebooks/02_linguistic_patterns.ipynb
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement readability analysis with deterministic output schema</name>
  <files>src/analysis/linguistic_quality.py, scripts/analyze_linguistic_quality.py</files>
  <action>Compute readability metrics (for example Flesch reading ease, grade-level metrics) on answer texts by label, generate aggregate tables, and produce consistently styled charts. Persist both raw and aggregated outputs for report flexibility.</action>
  <verify>`python scripts/analyze_linguistic_quality.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/analysis/phase3 --sections readability` writes readability tables and figures.</verify>
  <done>Readability differences by evasiveness label are reproducibly available in report-ready artifacts.</done>
</task>

<task type="auto">
  <name>Task 2: Add POS proportion and discourse-marker analysis</name>
  <files>src/analysis/linguistic_quality.py, scripts/analyze_linguistic_quality.py</files>
  <action>Use spaCy POS tagging to compute label-stratified POS proportions and implement discourse-marker frequency analysis (hedging and evasive cue lexicons). Emit normalized comparison tables, figures, and concise interpretation summaries.</action>
  <verify>`python scripts/analyze_linguistic_quality.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/analysis/phase3 --sections pos,discourse` produces POS/discourse outputs without manual intervention.</verify>
  <done>Linguistic style and discourse cues are captured with consistent, reproducible outputs by label.</done>
</task>

<task type="auto">
  <name>Task 3: Add tests and setup checks for linguistic pipeline reliability</name>
  <files>tests/test_linguistic_quality_analysis.py, docs/analysis_workflow.md</files>
  <action>Create tests for output schemas, normalization bounds, and deterministic behavior on fixed fixtures. Add explicit setup/troubleshooting docs for spaCy model availability and graceful failure messaging.</action>
  <verify>`pytest -q tests/test_linguistic_quality_analysis.py` passes and docs include setup + remediation commands for missing language model.</verify>
  <done>Linguistic-quality analysis is test-backed and operationally documented for reproducible execution.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -m spacy download en_core_web_sm`
- [ ] `python scripts/analyze_linguistic_quality.py --input data/processed/evasionbench_prepared.parquet --output-root artifacts/analysis/phase3`
- [ ] `pytest -q tests/test_linguistic_quality_analysis.py`
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- ANLY-04 readability/POS/discourse artifacts are reproducible and report-consumable
</success_criteria>

<output>
After completion, create `.planning/phases/03-statistical-linguistic-core-analyses/03-04-SUMMARY.md`
</output>
