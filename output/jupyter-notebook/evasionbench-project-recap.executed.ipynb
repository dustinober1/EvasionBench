{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recap-01",
   "metadata": {},
   "source": [
    "\n",
    "# EvasionBench Project Recap: Work, Results, and Decisions\n",
    "\n",
    "This notebook rebuilds a full project recap from repository evidence.\n",
    "\n",
    "It answers three questions:\n",
    "1. What did we build and run?\n",
    "2. What results did we get?\n",
    "3. Which decisions did we make, and why?\n",
    "\n",
    "All values come from files in `artifacts/`, `docs/`, and git history so the notebook stays reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-02",
   "metadata": {},
   "source": [
    "\n",
    "## Outline\n",
    "\n",
    "1. Repository + workflow snapshot\n",
    "2. Phase 3-4 analysis outputs\n",
    "3. Phase 5-6 modeling outputs\n",
    "4. Explainability and label diagnostics\n",
    "5. Phase 7 reporting status\n",
    "6. Phase 8 optimization and serving decision\n",
    "7. Decision log with evidence\n",
    "8. Final recap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recap-03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.513067Z",
     "iopub.status.busy": "2026-02-13T17:07:17.512910Z",
     "iopub.status.idle": "2026-02-13T17:07:17.528905Z",
     "shell.execute_reply": "2026-02-13T17:07:17.528438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/dustinober/Projects/EvasionBench\n",
      "Notebook run at (UTC): 2026-02-13T17:07:17.526618+00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"AGENTS.md\").exists() and (candidate / \"artifacts\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path.cwd())\n",
    "print(f\"Repo root: {ROOT}\")\n",
    "print(f\"Notebook run at (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "\n",
    "\n",
    "def read_json(rel_path: str, default: Any = None) -> Any:\n",
    "    path = ROOT / rel_path\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {rel_path}\")\n",
    "        return {} if default is None else default\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def read_csv(rel_path: str) -> list[dict[str, str]]:\n",
    "    path = ROOT / rel_path\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {rel_path}\")\n",
    "        return []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "\n",
    "def fmt(x: Any, digits: int = 4) -> str:\n",
    "    if isinstance(x, float):\n",
    "        return f\"{x:.{digits}f}\"\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def print_table(headers: list[str], rows: list[list[Any]], max_rows: int | None = None) -> None:\n",
    "    if max_rows is not None:\n",
    "        rows = rows[:max_rows]\n",
    "    widths = [len(h) for h in headers]\n",
    "    for row in rows:\n",
    "        for i, value in enumerate(row):\n",
    "            widths[i] = max(widths[i], len(fmt(value)))\n",
    "\n",
    "    def render_row(values: list[Any]) -> str:\n",
    "        return \" | \".join(fmt(v).ljust(widths[i]) for i, v in enumerate(values))\n",
    "\n",
    "    print(render_row(headers))\n",
    "    print(\"-+-\".join(\"-\" * w for w in widths))\n",
    "    for row in rows:\n",
    "        print(render_row(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-04",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Repository and Workflow Snapshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "recap-05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.532131Z",
     "iopub.status.busy": "2026-02-13T17:07:17.531821Z",
     "iopub.status.idle": "2026-02-13T17:07:17.558492Z",
     "shell.execute_reply": "2026-02-13T17:07:17.558066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script-first implementation footprint\n",
      "- script entrypoints (.py): 30\n",
      "- src modules (.py): 18\n",
      "- test files: 27\n",
      "\n",
      "Artifact footprint by phase\n",
      "path                            | exists | file_count\n",
      "--------------------------------+--------+-----------\n",
      "artifacts/analysis/phase3       | True   | 38        \n",
      "artifacts/analysis/phase4       | True   | 21        \n",
      "artifacts/models/phase5         | True   | 27        \n",
      "artifacts/models/phase6         | True   | 7         \n",
      "artifacts/explainability/phase6 | True   | 14        \n",
      "artifacts/diagnostics/phase6    | True   | 5         \n",
      "artifacts/reports/phase7        | True   | 8         \n",
      "artifacts/models/phase8         | True   | 9         \n",
      "\n",
      "Recent git timeline (most recent first)\n",
      "bb3b843 2026-02-13 feat: Generate new model optimization artifacts for phase8_3 and phase8_nonlogreg_tight.\n",
      "e15e693 2026-02-13 feat: Generate phase8_2b SVM model optimization artifacts and update the optimization script.\n",
      "fdedf9b 2026-02-13 feat: Add trained SVM model and associated evaluation artifacts for phase8_2.\n",
      "d294499 2026-02-13 feat: Introduce SVM model optimization and expand logistic regression search profiles, generating new model artifacts.\n",
      "ae22062 2026-02-13 feat: Implement model optimization across multiple random seeds, generating new seed-specific artifacts and updating the main phase8 model results.\n",
      "2e381ad 2026-02-12 feat: Generate phase 8 model optimization artifacts and update DVC/Makefile to remove explicit CV folds from the optimization command.\n",
      "3863c5d 2026-02-12 chore: remove temporary code coverage data.\n",
      "150a6e2 2026-02-12 feat: Introduce phase 8 model optimization, selection, and artifact verification pipeline with new scripts, tests, documentation, and dashboard integration.\n",
      "4f54c76 2026-02-12 feat: Improve confusion matrix loading to support alternative keys and handle missing labels gracefully.\n",
      "6d5b755 2026-02-12 feat: Introduce reviewer onboarding documentation, enhance `cleanlab` issue detection with fallback, and remove `.kilocode` agent rules.\n",
      "cfe1556 2026-02-12 feat: Add stop-slop skill for AI pattern removal\n",
      "be34b80 2026-02-12 chore: Delete the portfolio improvement plan document.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scripts_count = len(list((ROOT / \"scripts\").glob(\"*.py\")))\n",
    "src_modules = len(list((ROOT / \"src\").rglob(\"*.py\")))\n",
    "test_files = len(list((ROOT / \"tests\").glob(\"test_*.py\")))\n",
    "\n",
    "phase_dirs = [\n",
    "    \"artifacts/analysis/phase3\",\n",
    "    \"artifacts/analysis/phase4\",\n",
    "    \"artifacts/models/phase5\",\n",
    "    \"artifacts/models/phase6\",\n",
    "    \"artifacts/explainability/phase6\",\n",
    "    \"artifacts/diagnostics/phase6\",\n",
    "    \"artifacts/reports/phase7\",\n",
    "    \"artifacts/models/phase8\",\n",
    "]\n",
    "\n",
    "print(\"Script-first implementation footprint\")\n",
    "print(f\"- script entrypoints (.py): {scripts_count}\")\n",
    "print(f\"- src modules (.py): {src_modules}\")\n",
    "print(f\"- test files: {test_files}\")\n",
    "print()\n",
    "\n",
    "rows = []\n",
    "for rel in phase_dirs:\n",
    "    p = ROOT / rel\n",
    "    file_count = sum(1 for item in p.rglob(\"*\") if item.is_file()) if p.exists() else 0\n",
    "    rows.append([rel, p.exists(), file_count])\n",
    "\n",
    "print(\"Artifact footprint by phase\")\n",
    "print_table([\"path\", \"exists\", \"file_count\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Recent git timeline (most recent first)\")\n",
    "try:\n",
    "    log_out = subprocess.check_output(\n",
    "        [\"git\", \"log\", \"--pretty=format:%h %ad %s\", \"--date=short\", \"-n\", \"12\"],\n",
    "        cwd=ROOT,\n",
    "        text=True,\n",
    "    )\n",
    "    print(log_out)\n",
    "except Exception as exc:\n",
    "    print(f\"Could not read git log: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-06",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Phase 3-4 Analysis Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recap-07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.560312Z",
     "iopub.status.busy": "2026-02-13T17:07:17.560185Z",
     "iopub.status.idle": "2026-02-13T17:07:17.569091Z",
     "shell.execute_reply": "2026-02-13T17:07:17.568655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3: class distribution\n",
      "label         | count | share \n",
      "--------------+-------+-------\n",
      "direct        | 8749  | 52.31%\n",
      "intermediate  | 7359  | 44.00%\n",
      "fully_evasive | 618   | 3.69% \n",
      "\n",
      "Phase 3: length hypothesis tests\n",
      "- answer_length: p_value=0.000000 | Statistically significant differences detected across labels.\n",
      "- question_length: p_value=0.000000 | Statistically significant differences detected across labels.\n",
      "\n",
      "Phase 4: semantic hypothesis summary\n",
      "- H1: relevance_deflection -> Highest alignment label: direct (0.0800); lowest alignment label: fully_evasive (0.0526); delta=0.0274.\n",
      "- H2: vague_response_behavior -> Similarity standard deviations are reported per label for interpretation.\n",
      "\n",
      "Phase 4: question behavior refusal spread (top 5)\n",
      "question_type | min_refusal | max_refusal | spread\n",
      "--------------+-------------+-------------+-------\n",
      "yes_no        | 0.0615      | 0.2593      | 0.1978\n",
      "factual       | 0.0643      | 0.2407      | 0.1764\n",
      "opinion       | 0.0799      | 0.2432      | 0.1633\n",
      "multi_part    | 0.0955      | 0.2356      | 0.1401\n",
      "other         | 0.0675      | 0.1857      | 0.1182\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_dist = read_json(\"artifacts/analysis/phase3/core_stats/class_distribution.json\", default=[])\n",
    "length_tests = read_json(\"artifacts/analysis/phase3/core_stats/length_tests.json\", default={})\n",
    "semantic_h = read_json(\"artifacts/analysis/phase4/semantic_similarity/hypothesis_summary.json\", default={})\n",
    "question_behavior = read_json(\"artifacts/analysis/phase4/question_behavior/question_behavior_summary.json\", default={})\n",
    "\n",
    "print(\"Phase 3: class distribution\")\n",
    "total = sum(item.get(\"count\", 0) for item in class_dist) or 1\n",
    "rows = []\n",
    "for item in class_dist:\n",
    "    label = item.get(\"label\", \"unknown\")\n",
    "    count = item.get(\"count\", 0)\n",
    "    pct = 100.0 * count / total\n",
    "    rows.append([label, count, f\"{pct:.2f}%\"])\n",
    "print_table([\"label\", \"count\", \"share\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Phase 3: length hypothesis tests\")\n",
    "for metric_name in [\"answer_length\", \"question_length\"]:\n",
    "    block = length_tests.get(metric_name, {})\n",
    "    kruskal = block.get(\"kruskal\", {})\n",
    "    p_value = kruskal.get(\"p_value\")\n",
    "    interpretation = block.get(\"interpretation\", \"\")\n",
    "    print(f\"- {metric_name}: p_value={fmt(p_value, 6)} | {interpretation}\")\n",
    "\n",
    "print()\n",
    "print(\"Phase 4: semantic hypothesis summary\")\n",
    "for item in semantic_h.get(\"hypotheses\", []):\n",
    "    print(f\"- {item.get('id')}: {item.get('name')} -> {item.get('finding')}\")\n",
    "\n",
    "print()\n",
    "print(\"Phase 4: question behavior refusal spread (top 5)\")\n",
    "spreads = question_behavior.get(\"refusal_spread_by_question_type\", [])\n",
    "spreads_sorted = sorted(spreads, key=lambda x: x.get(\"refusal_rate_spread\", 0), reverse=True)\n",
    "rows = []\n",
    "for item in spreads_sorted[:5]:\n",
    "    rows.append([\n",
    "        item.get(\"question_type\"),\n",
    "        item.get(\"min_refusal_rate\"),\n",
    "        item.get(\"max_refusal_rate\"),\n",
    "        item.get(\"refusal_rate_spread\"),\n",
    "    ])\n",
    "print_table([\"question_type\", \"min_refusal\", \"max_refusal\", \"spread\"], rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-08",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Phase 5-6 Modeling Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recap-09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.571194Z",
     "iopub.status.busy": "2026-02-13T17:07:17.571053Z",
     "iopub.status.idle": "2026-02-13T17:07:17.578882Z",
     "shell.execute_reply": "2026-02-13T17:07:17.578594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 5 canonical summary (run_summary.json)\n",
      "family   | accuracy | f1_macro | precision_macro | recall_macro\n",
      "---------+----------+----------+-----------------+-------------\n",
      "boosting | 0.6088   | 0.4496   | 0.6259          | 0.4389      \n",
      "logreg   | 0.6432   | 0.5343   | 0.6566          | 0.5050      \n",
      "tree     | 0.5870   | 0.4355   | 0.6929          | 0.4223      \n",
      "\n",
      "Best classical family by macro-F1 (from summary): logreg (0.5343)\n",
      "\n",
      "Phase 5 direct metrics files (for sanity check)\n",
      "family        | accuracy | f1_macro | precision_macro | recall_macro\n",
      "--------------+----------+----------+-----------------+-------------\n",
      "logreg        | 0.5000   | 0.3333   | 0.2500          | 0.5000      \n",
      "tree          | 0.5870   | 0.4355   | 0.6929          | 0.4223      \n",
      "boosting      | 0.6088   | 0.4496   | 0.6259          | 0.4389      \n",
      "tree_boosting | 0.5882   | 0.4356   | 0.6621          | 0.4229      \n",
      "\n",
      "Phase 6 transformer metrics\n",
      "{'accuracy': 0.5, 'f1_macro': 0.3333333333333333, 'precision_macro': 0.25, 'recall_macro': 0.5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phase5_summary = read_json(\"artifacts/models/phase5/run_summary.json\", default={})\n",
    "phase5_metrics_files = {\n",
    "    \"logreg\": read_json(\"artifacts/models/phase5/logreg/metrics.json\", default={}),\n",
    "    \"tree\": read_json(\"artifacts/models/phase5/tree/metrics.json\", default={}),\n",
    "    \"boosting\": read_json(\"artifacts/models/phase5/boosting/metrics.json\", default={}),\n",
    "    \"tree_boosting\": read_json(\"artifacts/models/phase5/tree_boosting/metrics.json\", default={}),\n",
    "}\n",
    "phase6_transformer = read_json(\"artifacts/models/phase6/transformer/metrics.json\", default={})\n",
    "\n",
    "print(\"Phase 5 canonical summary (run_summary.json)\")\n",
    "rows = []\n",
    "for family, metrics in sorted(phase5_summary.items()):\n",
    "    rows.append([\n",
    "        family,\n",
    "        metrics.get(\"accuracy\"),\n",
    "        metrics.get(\"f1_macro\"),\n",
    "        metrics.get(\"precision_macro\"),\n",
    "        metrics.get(\"recall_macro\"),\n",
    "    ])\n",
    "print_table([\"family\", \"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"], rows)\n",
    "\n",
    "if rows:\n",
    "    best = max(rows, key=lambda r: r[2] if isinstance(r[2], float) else -1)\n",
    "    print()\n",
    "    print(f\"Best classical family by macro-F1 (from summary): {best[0]} ({fmt(best[2])})\")\n",
    "\n",
    "print()\n",
    "print(\"Phase 5 direct metrics files (for sanity check)\")\n",
    "rows = []\n",
    "for family, metrics in phase5_metrics_files.items():\n",
    "    rows.append([\n",
    "        family,\n",
    "        metrics.get(\"accuracy\"),\n",
    "        metrics.get(\"f1_macro\"),\n",
    "        metrics.get(\"precision_macro\"),\n",
    "        metrics.get(\"recall_macro\"),\n",
    "    ])\n",
    "print_table([\"family\", \"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Phase 6 transformer metrics\")\n",
    "print(phase6_transformer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-10",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Explainability and Label Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "recap-11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.580496Z",
     "iopub.status.busy": "2026-02-13T17:07:17.580336Z",
     "iopub.status.idle": "2026-02-13T17:07:17.588271Z",
     "shell.execute_reply": "2026-02-13T17:07:17.587891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability families and explainers\n",
      "family   | explainer       | n_features | n_samples\n",
      "---------+-----------------+------------+----------\n",
      "boosting | TreeExplainer   | 80         | 10       \n",
      "logreg   | LinearExplainer | 268        | 10       \n",
      "tree     | TreeExplainer   | 268        | 10       \n",
      "\n",
      "Label diagnostics summary\n",
      "- training_size: 80\n",
      "- quality_score: 100.0\n",
      "- label_issues: 0\n",
      "- near_duplicate_issues: 80\n",
      "- outlier_issues: 0\n",
      "- random_state: 42\n",
      "- git_sha: 2be7541\n",
      "\n",
      "Raw diagnostics file row counts\n",
      "- near_duplicate_pairs.csv rows: 80\n",
      "- suspect_examples.csv rows: 0\n",
      "- outlier_examples.csv rows: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xai_summary = read_json(\"artifacts/explainability/phase6/xai_summary.json\", default={})\n",
    "label_diag = read_json(\"artifacts/diagnostics/phase6/label_diagnostics_summary.json\", default={})\n",
    "near_dupes = read_csv(\"artifacts/diagnostics/phase6/near_duplicate_pairs.csv\")\n",
    "suspects = read_csv(\"artifacts/diagnostics/phase6/suspect_examples.csv\")\n",
    "outliers = read_csv(\"artifacts/diagnostics/phase6/outlier_examples.csv\")\n",
    "\n",
    "print(\"Explainability families and explainers\")\n",
    "rows = []\n",
    "for family, payload in sorted(xai_summary.items()):\n",
    "    rows.append([\n",
    "        family,\n",
    "        payload.get(\"explainer_type\"),\n",
    "        payload.get(\"n_features\"),\n",
    "        payload.get(\"n_samples\"),\n",
    "    ])\n",
    "print_table([\"family\", \"explainer\", \"n_features\", \"n_samples\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Label diagnostics summary\")\n",
    "for k in [\n",
    "    \"training_size\",\n",
    "    \"quality_score\",\n",
    "    \"label_issues\",\n",
    "    \"near_duplicate_issues\",\n",
    "    \"outlier_issues\",\n",
    "    \"random_state\",\n",
    "    \"git_sha\",\n",
    "]:\n",
    "    print(f\"- {k}: {label_diag.get(k)}\")\n",
    "\n",
    "print()\n",
    "print(\"Raw diagnostics file row counts\")\n",
    "print(f\"- near_duplicate_pairs.csv rows: {len(near_dupes)}\")\n",
    "print(f\"- suspect_examples.csv rows: {len(suspects)}\")\n",
    "print(f\"- outlier_examples.csv rows: {len(outliers)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-12",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Phase 7 Reporting Pipeline Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recap-13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.591894Z",
     "iopub.status.busy": "2026-02-13T17:07:17.591731Z",
     "iopub.status.idle": "2026-02-13T17:07:17.599258Z",
     "shell.execute_reply": "2026-02-13T17:07:17.597335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 7 run summary\n",
      "- pipeline: phase7_research_reporting\n",
      "- status: failed\n",
      "- started_at: 2026-02-11T00:32:24Z\n",
      "- completed_at: 2026-02-11T00:32:49Z\n",
      "- git_sha: 678faa5\n",
      "\n",
      "Failure detail (if any)\n",
      "- stage: phase3_analysis\n",
      "- status: failed\n",
      "- exit_code: 1\n",
      "- hint: Run missing NLP prerequisites and retry phase3 analysis.\n",
      "- log: artifacts/reports/phase7/logs/01_phase3_analysis.log\n",
      "\n",
      "Manifest coverage\n",
      "- analyses tracked: 43\n",
      "- models tracked: 28\n",
      "- explainability tracked: 18\n",
      "- diagnostics tracked: 5\n",
      "- traceability item count: 84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "report_run = read_json(\"artifacts/reports/phase7/run_summary.json\", default={})\n",
    "manifest = read_json(\"artifacts/reports/phase7/provenance_manifest.json\", default={})\n",
    "traceability = read_json(\"artifacts/reports/phase7/report_traceability.json\", default={})\n",
    "\n",
    "print(\"Phase 7 run summary\")\n",
    "for k in [\"pipeline\", \"status\", \"started_at\", \"completed_at\", \"git_sha\"]:\n",
    "    print(f\"- {k}: {report_run.get(k)}\")\n",
    "\n",
    "failure = report_run.get(\"failure\") or {}\n",
    "if failure:\n",
    "    print()\n",
    "    print(\"Failure detail (if any)\")\n",
    "    for k in [\"stage\", \"status\", \"exit_code\", \"hint\", \"log\"]:\n",
    "        print(f\"- {k}: {failure.get(k)}\")\n",
    "\n",
    "sections = manifest.get(\"sections\", {})\n",
    "analysis_count = len(sections.get(\"analyses\", []))\n",
    "model_count = len(sections.get(\"models\", []))\n",
    "explainability_count = len(sections.get(\"explainability\", []))\n",
    "diagnostics_count = len(sections.get(\"diagnostics\", []))\n",
    "\n",
    "print()\n",
    "print(\"Manifest coverage\")\n",
    "print(f\"- analyses tracked: {analysis_count}\")\n",
    "print(f\"- models tracked: {model_count}\")\n",
    "print(f\"- explainability tracked: {explainability_count}\")\n",
    "print(f\"- diagnostics tracked: {diagnostics_count}\")\n",
    "\n",
    "trace_items = traceability.get(\"items\", {})\n",
    "print(f\"- traceability item count: {len(trace_items)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-14",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Phase 8 Optimization and Serving Decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recap-15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.601697Z",
     "iopub.status.busy": "2026-02-13T17:07:17.601551Z",
     "iopub.status.idle": "2026-02-13T17:07:17.611676Z",
     "shell.execute_reply": "2026-02-13T17:07:17.611202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization runs with selected_model.json\n",
      "run                    | family | winner_trial_id | winner_rule             | acc_floor | accuracy | f1_macro | seed\n",
      "-----------------------+--------+-----------------+-------------------------+-----------+----------+----------+-----\n",
      "phase8                 | logreg | logreg_0059     | accuracy_floor_enforced | 0.6300    | 0.6309   | 0.5522   | 84  \n",
      "phase8_1               | logreg | logreg_0049     | accuracy_floor_enforced | 0.6400    | 0.6411   | 0.5291   | 42  \n",
      "phase8_2               | svm    | svm_0001        | accuracy_floor_relaxed  | 0.6400    | 0.6360   | 0.4924   | 42  \n",
      "phase8_2b              | svm    | svm_0007        | accuracy_floor_enforced | 0.6400    | 0.6458   | 0.5039   | 42  \n",
      "phase8_3               | sgd    | sgd_0020        | accuracy_floor_relaxed  | 0.6400    | 0.6369   | 0.5535   | 42  \n",
      "phase8_nonlogreg_tight | sgd    | sgd_0020        | accuracy_floor_relaxed  | 0.6400    | 0.6369   | 0.5535   | 42  \n",
      "phase8_pruned2         | logreg | logreg_0004     | accuracy_floor_enforced | 0.6432    | 0.6441   | 0.4965   | 42  \n",
      "phase8_seed21          | logreg | logreg_0062     | accuracy_floor_enforced | 0.6300    | 0.6360   | 0.5392   | 21  \n",
      "phase8_seed42          | logreg | logreg_0056     | accuracy_floor_enforced | 0.6300    | 0.6411   | 0.5291   | 42  \n",
      "phase8_seed84          | logreg | logreg_0059     | accuracy_floor_enforced | 0.6300    | 0.6309   | 0.5522   | 84  \n",
      "\n",
      "Canonical serving selection (artifacts/models/phase8/selected_model.json)\n",
      "- best_model_family: logreg\n",
      "- winner_trial_id: logreg_0059\n",
      "- winner_rule: accuracy_floor_enforced\n",
      "- accuracy_floor: 0.63\n",
      "- accuracy: 0.6309025702331141\n",
      "- f1_macro: 0.5521623974373844\n",
      "\n",
      "Seed-run macro-F1 mean: 0.5402\n",
      "Seed-run macro-F1 stdev: 0.0094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phase8_selected = sorted((ROOT / \"artifacts\" / \"models\").glob(\"phase8*/selected_model.json\"))\n",
    "rows = []\n",
    "for path in phase8_selected:\n",
    "    rel = path.relative_to(ROOT).as_posix()\n",
    "    payload = read_json(rel, default={})\n",
    "    metrics = payload.get(\"metrics\", {})\n",
    "    rows.append([\n",
    "        path.parent.name,\n",
    "        payload.get(\"best_model_family\"),\n",
    "        payload.get(\"winner_trial_id\"),\n",
    "        payload.get(\"winner_rule\"),\n",
    "        payload.get(\"accuracy_floor\"),\n",
    "        metrics.get(\"accuracy\"),\n",
    "        metrics.get(\"f1_macro\"),\n",
    "        payload.get(\"random_state\"),\n",
    "    ])\n",
    "\n",
    "print(\"Optimization runs with selected_model.json\")\n",
    "print_table(\n",
    "    [\n",
    "        \"run\",\n",
    "        \"family\",\n",
    "        \"winner_trial_id\",\n",
    "        \"winner_rule\",\n",
    "        \"acc_floor\",\n",
    "        \"accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"seed\",\n",
    "    ],\n",
    "    rows,\n",
    ")\n",
    "\n",
    "canonical = read_json(\"artifacts/models/phase8/selected_model.json\", default={})\n",
    "canon_metrics = canonical.get(\"metrics\", {})\n",
    "print()\n",
    "print(\"Canonical serving selection (artifacts/models/phase8/selected_model.json)\")\n",
    "print(f\"- best_model_family: {canonical.get('best_model_family')}\")\n",
    "print(f\"- winner_trial_id: {canonical.get('winner_trial_id')}\")\n",
    "print(f\"- winner_rule: {canonical.get('winner_rule')}\")\n",
    "print(f\"- accuracy_floor: {canonical.get('accuracy_floor')}\")\n",
    "print(f\"- accuracy: {canon_metrics.get('accuracy')}\")\n",
    "print(f\"- f1_macro: {canon_metrics.get('f1_macro')}\")\n",
    "\n",
    "seed_rows = [r for r in rows if r[0].startswith(\"phase8_seed\") and isinstance(r[6], float)]\n",
    "if seed_rows:\n",
    "    f1_values = [r[6] for r in seed_rows]\n",
    "    print()\n",
    "    print(f\"Seed-run macro-F1 mean: {statistics.mean(f1_values):.4f}\")\n",
    "    print(f\"Seed-run macro-F1 stdev: {statistics.pstdev(f1_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-16",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Decision Log (What We Chose and Why)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "recap-17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.613534Z",
     "iopub.status.busy": "2026-02-13T17:07:17.613393Z",
     "iopub.status.idle": "2026-02-13T17:07:17.621110Z",
     "shell.execute_reply": "2026-02-13T17:07:17.620821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decisions and rationale\n",
      "1. Script-first workflow\n",
      "   - rationale: All executable logic moved to scripts/src and phase outputs are contract-tested.\n",
      "   - Evidence: AGENTS.md + docs/analysis_workflow.md\n",
      "2. Use question [SEP] answer representation\n",
      "   - rationale: Feature strategy ties question-answer context into one text stream for TF-IDF pipelines.\n",
      "   - Evidence: AGENTS.md and docs/architecture.md\n",
      "3. Classical baseline emphasis first\n",
      "   - rationale: Interpretable, fast baselines established stable benchmark behavior before advanced modeling.\n",
      "   - Evidence: docs/case_study.md + docs/hiring_manager_brief.md\n",
      "4. Optimize with macro-F1 + accuracy floor\n",
      "   - rationale: Selection policy protects minority-class quality while preventing unacceptable accuracy regression.\n",
      "   - Evidence: docs/model_optimization_guide.md + phase8 selected_model.json (floor=0.63, f1=0.5521623974373844)\n",
      "5. Serve via canonical selected_model.json\n",
      "   - rationale: Runtime resolution uses a deterministic selected-model file for reproducible deployment.\n",
      "   - Evidence: docs/model_optimization_guide.md serving default section\n",
      "\n",
      "Cross-check: exact phrase presence in docs\n",
      "- case_study_has_logreg_why: True\n",
      "- brief_has_tradeoffs: True\n",
      "- opt_has_accuracy_floor: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "case_study_text = (ROOT / \"docs\" / \"case_study.md\").read_text(encoding=\"utf-8\") if (ROOT / \"docs\" / \"case_study.md\").exists() else \"\"\n",
    "brief_text = (ROOT / \"docs\" / \"hiring_manager_brief.md\").read_text(encoding=\"utf-8\") if (ROOT / \"docs\" / \"hiring_manager_brief.md\").exists() else \"\"\n",
    "opt_text = (ROOT / \"docs\" / \"model_optimization_guide.md\").read_text(encoding=\"utf-8\") if (ROOT / \"docs\" / \"model_optimization_guide.md\").exists() else \"\"\n",
    "\n",
    "canonical = read_json(\"artifacts/models/phase8/selected_model.json\", default={})\n",
    "canon_metrics = canonical.get(\"metrics\", {})\n",
    "\n",
    "decisions = [\n",
    "    (\n",
    "        \"Script-first workflow\",\n",
    "        \"All executable logic moved to scripts/src and phase outputs are contract-tested.\",\n",
    "        \"Evidence: AGENTS.md + docs/analysis_workflow.md\",\n",
    "    ),\n",
    "    (\n",
    "        \"Use question [SEP] answer representation\",\n",
    "        \"Feature strategy ties question-answer context into one text stream for TF-IDF pipelines.\",\n",
    "        \"Evidence: AGENTS.md and docs/architecture.md\",\n",
    "    ),\n",
    "    (\n",
    "        \"Classical baseline emphasis first\",\n",
    "        \"Interpretable, fast baselines established stable benchmark behavior before advanced modeling.\",\n",
    "        \"Evidence: docs/case_study.md + docs/hiring_manager_brief.md\",\n",
    "    ),\n",
    "    (\n",
    "        \"Optimize with macro-F1 + accuracy floor\",\n",
    "        \"Selection policy protects minority-class quality while preventing unacceptable accuracy regression.\",\n",
    "        f\"Evidence: docs/model_optimization_guide.md + phase8 selected_model.json (floor={canonical.get('accuracy_floor')}, f1={canon_metrics.get('f1_macro')})\",\n",
    "    ),\n",
    "    (\n",
    "        \"Serve via canonical selected_model.json\",\n",
    "        \"Runtime resolution uses a deterministic selected-model file for reproducible deployment.\",\n",
    "        \"Evidence: docs/model_optimization_guide.md serving default section\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Decisions and rationale\")\n",
    "for i, (decision, rationale, evidence) in enumerate(decisions, start=1):\n",
    "    print(f\"{i}. {decision}\")\n",
    "    print(f\"   - rationale: {rationale}\")\n",
    "    print(f\"   - {evidence}\")\n",
    "\n",
    "print()\n",
    "print(\"Cross-check: exact phrase presence in docs\")\n",
    "checks = [\n",
    "    (\"case_study_has_logreg_why\", \"Why Logistic Regression won\" in case_study_text),\n",
    "    (\"brief_has_tradeoffs\", \"Engineering Decisions and Tradeoffs\" in brief_text),\n",
    "    (\"opt_has_accuracy_floor\", \"Accuracy floor\" in opt_text),\n",
    "]\n",
    "for key, passed in checks:\n",
    "    print(f\"- {key}: {passed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap-18",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Final Recap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recap-19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T17:07:17.624652Z",
     "iopub.status.busy": "2026-02-13T17:07:17.624468Z",
     "iopub.status.idle": "2026-02-13T17:07:17.629193Z",
     "shell.execute_reply": "2026-02-13T17:07:17.628824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project recap summary\n",
      "- project: EvasionBench\n",
      "- workflow: script-first pipeline with artifact contracts\n",
      "- phase5_best_classical_family: logreg\n",
      "- phase8_canonical_family: logreg\n",
      "- phase8_canonical_trial: logreg_0059\n",
      "- phase8_canonical_accuracy: 0.6309025702331141\n",
      "- phase8_canonical_f1_macro: 0.5521623974373844\n",
      "- label_quality_score: 100.0\n",
      "- phase7_report_status: failed\n",
      "\n",
      "Notebook complete: this run reconstructed work, outcomes, and decisions from repository evidence.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phase5_summary = read_json(\"artifacts/models/phase5/run_summary.json\", default={})\n",
    "canonical = read_json(\"artifacts/models/phase8/selected_model.json\", default={})\n",
    "report_run = read_json(\"artifacts/reports/phase7/run_summary.json\", default={})\n",
    "label_diag = read_json(\"artifacts/diagnostics/phase6/label_diagnostics_summary.json\", default={})\n",
    "\n",
    "phase5_best = None\n",
    "if phase5_summary:\n",
    "    phase5_best = max(\n",
    "        phase5_summary.items(),\n",
    "        key=lambda kv: kv[1].get(\"f1_macro\", float(\"-inf\")),\n",
    "    )[0]\n",
    "\n",
    "recap = {\n",
    "    \"project\": \"EvasionBench\",\n",
    "    \"workflow\": \"script-first pipeline with artifact contracts\",\n",
    "    \"phase5_best_classical_family\": phase5_best,\n",
    "    \"phase8_canonical_family\": canonical.get(\"best_model_family\"),\n",
    "    \"phase8_canonical_trial\": canonical.get(\"winner_trial_id\"),\n",
    "    \"phase8_canonical_accuracy\": canonical.get(\"metrics\", {}).get(\"accuracy\"),\n",
    "    \"phase8_canonical_f1_macro\": canonical.get(\"metrics\", {}).get(\"f1_macro\"),\n",
    "    \"label_quality_score\": label_diag.get(\"quality_score\"),\n",
    "    \"phase7_report_status\": report_run.get(\"status\"),\n",
    "}\n",
    "\n",
    "print(\"Project recap summary\")\n",
    "for k, v in recap.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "print()\n",
    "print(\"Notebook complete: this run reconstructed work, outcomes, and decisions from repository evidence.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
