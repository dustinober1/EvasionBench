{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# EvasionBench Project Recap: Work, Results, and Decisions\n",
    "\n",
    "This notebook rebuilds a full project recap from repository evidence.\n",
    "\n",
    "It answers three questions:\n",
    "1. What did we build and run?\n",
    "2. What results did we get?\n",
    "3. Which decisions did we make, and why?\n",
    "\n",
    "All values come from files in `artifacts/`, `docs/`, and git history so the notebook stays reproducible.\n"
   ],
   "id": "recap-01"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Outline\n",
    "\n",
    "1. Repository + workflow snapshot\n",
    "2. Phase 3-4 analysis outputs\n",
    "3. Phase 5-6 modeling outputs\n",
    "4. Explainability and label diagnostics\n",
    "5. Phase 7 reporting status\n",
    "6. Phase 8 optimization and serving decision\n",
    "7. Decision log with evidence\n",
    "8. Final recap\n"
   ],
   "id": "recap-02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "import subprocess\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for candidate in [start, *start.parents]:\n",
    "        if (candidate / \"AGENTS.md\").exists() and (candidate / \"artifacts\").exists():\n",
    "            return candidate\n",
    "    return start\n",
    "\n",
    "\n",
    "ROOT = find_repo_root(Path.cwd())\n",
    "print(f\"Repo root: {ROOT}\")\n",
    "print(f\"Notebook run at (UTC): {datetime.now(timezone.utc).isoformat()}\")\n",
    "\n",
    "\n",
    "def read_json(rel_path: str, default: Any = None) -> Any:\n",
    "    path = ROOT / rel_path\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {rel_path}\")\n",
    "        return {} if default is None else default\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def read_csv(rel_path: str) -> list[dict[str, str]]:\n",
    "    path = ROOT / rel_path\n",
    "    if not path.exists():\n",
    "        print(f\"[missing] {rel_path}\")\n",
    "        return []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        return list(csv.DictReader(f))\n",
    "\n",
    "\n",
    "def fmt(x: Any, digits: int = 4) -> str:\n",
    "    if isinstance(x, float):\n",
    "        return f\"{x:.{digits}f}\"\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def print_table(headers: list[str], rows: list[list[Any]], max_rows: int | None = None) -> None:\n",
    "    if max_rows is not None:\n",
    "        rows = rows[:max_rows]\n",
    "    widths = [len(h) for h in headers]\n",
    "    for row in rows:\n",
    "        for i, value in enumerate(row):\n",
    "            widths[i] = max(widths[i], len(fmt(value)))\n",
    "\n",
    "    def render_row(values: list[Any]) -> str:\n",
    "        return \" | \".join(fmt(v).ljust(widths[i]) for i, v in enumerate(values))\n",
    "\n",
    "    print(render_row(headers))\n",
    "    print(\"-+-\".join(\"-\" * w for w in widths))\n",
    "    for row in rows:\n",
    "        print(render_row(row))\n"
   ],
   "id": "recap-03"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Repository and Workflow Snapshot\n"
   ],
   "id": "recap-04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scripts_count = len(list((ROOT / \"scripts\").glob(\"*.py\")))\n",
    "src_modules = len(list((ROOT / \"src\").rglob(\"*.py\")))\n",
    "test_files = len(list((ROOT / \"tests\").glob(\"test_*.py\")))\n",
    "\n",
    "phase_dirs = [\n",
    "    \"artifacts/analysis/phase3\",\n",
    "    \"artifacts/analysis/phase4\",\n",
    "    \"artifacts/models/phase5\",\n",
    "    \"artifacts/models/phase6\",\n",
    "    \"artifacts/explainability/phase6\",\n",
    "    \"artifacts/diagnostics/phase6\",\n",
    "    \"artifacts/reports/phase7\",\n",
    "    \"artifacts/models/phase8\",\n",
    "]\n",
    "\n",
    "print(\"Script-first implementation footprint\")\n",
    "print(f\"- script entrypoints (.py): {scripts_count}\")\n",
    "print(f\"- src modules (.py): {src_modules}\")\n",
    "print(f\"- test files: {test_files}\")\n",
    "print()\n",
    "\n",
    "rows = []\n",
    "for rel in phase_dirs:\n",
    "    p = ROOT / rel\n",
    "    file_count = sum(1 for item in p.rglob(\"*\") if item.is_file()) if p.exists() else 0\n",
    "    rows.append([rel, p.exists(), file_count])\n",
    "\n",
    "print(\"Artifact footprint by phase\")\n",
    "print_table([\"path\", \"exists\", \"file_count\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Recent git timeline (most recent first)\")\n",
    "try:\n",
    "    log_out = subprocess.check_output(\n",
    "        [\"git\", \"log\", \"--pretty=format:%h %ad %s\", \"--date=short\", \"-n\", \"12\"],\n",
    "        cwd=ROOT,\n",
    "        text=True,\n",
    "    )\n",
    "    print(log_out)\n",
    "except Exception as exc:\n",
    "    print(f\"Could not read git log: {exc}\")\n"
   ],
   "id": "recap-05"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Phase 3-4 Analysis Results\n"
   ],
   "id": "recap-06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_dist = read_json(\"artifacts/analysis/phase3/core_stats/class_distribution.json\", default=[])\n",
    "length_tests = read_json(\"artifacts/analysis/phase3/core_stats/length_tests.json\", default={})\n",
    "semantic_h = read_json(\"artifacts/analysis/phase4/semantic_similarity/hypothesis_summary.json\", default={})\n",
    "question_behavior = read_json(\"artifacts/analysis/phase4/question_behavior/question_behavior_summary.json\", default={})\n",
    "\n",
    "print(\"Phase 3: class distribution\")\n",
    "total = sum(item.get(\"count\", 0) for item in class_dist) or 1\n",
    "rows = []\n",
    "for item in class_dist:\n",
    "    label = item.get(\"label\", \"unknown\")\n",
    "    count = item.get(\"count\", 0)\n",
    "    pct = 100.0 * count / total\n",
    "    rows.append([label, count, f\"{pct:.2f}%\"])\n",
    "print_table([\"label\", \"count\", \"share\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Phase 3: length hypothesis tests\")\n",
    "for metric_name in [\"answer_length\", \"question_length\"]:\n",
    "    block = length_tests.get(metric_name, {})\n",
    "    kruskal = block.get(\"kruskal\", {})\n",
    "    p_value = kruskal.get(\"p_value\")\n",
    "    interpretation = block.get(\"interpretation\", \"\")\n",
    "    print(f\"- {metric_name}: p_value={fmt(p_value, 6)} | {interpretation}\")\n",
    "\n",
    "print()\n",
    "print(\"Phase 4: semantic hypothesis summary\")\n",
    "for item in semantic_h.get(\"hypotheses\", []):\n",
    "    print(f\"- {item.get('id')}: {item.get('name')} -> {item.get('finding')}\")\n",
    "\n",
    "print()\n",
    "print(\"Phase 4: question behavior refusal spread (top 5)\")\n",
    "spreads = question_behavior.get(\"refusal_spread_by_question_type\", [])\n",
    "spreads_sorted = sorted(spreads, key=lambda x: x.get(\"refusal_rate_spread\", 0), reverse=True)\n",
    "rows = []\n",
    "for item in spreads_sorted[:5]:\n",
    "    rows.append([\n",
    "        item.get(\"question_type\"),\n",
    "        item.get(\"min_refusal_rate\"),\n",
    "        item.get(\"max_refusal_rate\"),\n",
    "        item.get(\"refusal_rate_spread\"),\n",
    "    ])\n",
    "print_table([\"question_type\", \"min_refusal\", \"max_refusal\", \"spread\"], rows)\n"
   ],
   "id": "recap-07"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Phase 5-6 Modeling Results\n"
   ],
   "id": "recap-08"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phase5_summary = read_json(\"artifacts/models/phase5/run_summary.json\", default={})\n",
    "phase5_metrics_files = {\n",
    "    \"logreg\": read_json(\"artifacts/models/phase5/logreg/metrics.json\", default={}),\n",
    "    \"tree\": read_json(\"artifacts/models/phase5/tree/metrics.json\", default={}),\n",
    "    \"boosting\": read_json(\"artifacts/models/phase5/boosting/metrics.json\", default={}),\n",
    "    \"tree_boosting\": read_json(\"artifacts/models/phase5/tree_boosting/metrics.json\", default={}),\n",
    "}\n",
    "phase6_transformer = read_json(\"artifacts/models/phase6/transformer/metrics.json\", default={})\n",
    "\n",
    "print(\"Phase 5 canonical summary (run_summary.json)\")\n",
    "rows = []\n",
    "for family, metrics in sorted(phase5_summary.items()):\n",
    "    rows.append([\n",
    "        family,\n",
    "        metrics.get(\"accuracy\"),\n",
    "        metrics.get(\"f1_macro\"),\n",
    "        metrics.get(\"precision_macro\"),\n",
    "        metrics.get(\"recall_macro\"),\n",
    "    ])\n",
    "print_table([\"family\", \"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"], rows)\n",
    "\n",
    "if rows:\n",
    "    best = max(rows, key=lambda r: r[2] if isinstance(r[2], float) else -1)\n",
    "    print()\n",
    "    print(f\"Best classical family by macro-F1 (from summary): {best[0]} ({fmt(best[2])})\")\n",
    "\n",
    "print()\n",
    "print(\"Phase 5 direct metrics files (for sanity check)\")\n",
    "rows = []\n",
    "for family, metrics in phase5_metrics_files.items():\n",
    "    rows.append([\n",
    "        family,\n",
    "        metrics.get(\"accuracy\"),\n",
    "        metrics.get(\"f1_macro\"),\n",
    "        metrics.get(\"precision_macro\"),\n",
    "        metrics.get(\"recall_macro\"),\n",
    "    ])\n",
    "print_table([\"family\", \"accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Phase 6 transformer metrics\")\n",
    "print(phase6_transformer)\n"
   ],
   "id": "recap-09"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Explainability and Label Quality\n"
   ],
   "id": "recap-10"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xai_summary = read_json(\"artifacts/explainability/phase6/xai_summary.json\", default={})\n",
    "label_diag = read_json(\"artifacts/diagnostics/phase6/label_diagnostics_summary.json\", default={})\n",
    "near_dupes = read_csv(\"artifacts/diagnostics/phase6/near_duplicate_pairs.csv\")\n",
    "suspects = read_csv(\"artifacts/diagnostics/phase6/suspect_examples.csv\")\n",
    "outliers = read_csv(\"artifacts/diagnostics/phase6/outlier_examples.csv\")\n",
    "\n",
    "print(\"Explainability families and explainers\")\n",
    "rows = []\n",
    "for family, payload in sorted(xai_summary.items()):\n",
    "    rows.append([\n",
    "        family,\n",
    "        payload.get(\"explainer_type\"),\n",
    "        payload.get(\"n_features\"),\n",
    "        payload.get(\"n_samples\"),\n",
    "    ])\n",
    "print_table([\"family\", \"explainer\", \"n_features\", \"n_samples\"], rows)\n",
    "\n",
    "print()\n",
    "print(\"Label diagnostics summary\")\n",
    "for k in [\n",
    "    \"training_size\",\n",
    "    \"quality_score\",\n",
    "    \"label_issues\",\n",
    "    \"near_duplicate_issues\",\n",
    "    \"outlier_issues\",\n",
    "    \"random_state\",\n",
    "    \"git_sha\",\n",
    "]:\n",
    "    print(f\"- {k}: {label_diag.get(k)}\")\n",
    "\n",
    "print()\n",
    "print(\"Raw diagnostics file row counts\")\n",
    "print(f\"- near_duplicate_pairs.csv rows: {len(near_dupes)}\")\n",
    "print(f\"- suspect_examples.csv rows: {len(suspects)}\")\n",
    "print(f\"- outlier_examples.csv rows: {len(outliers)}\")\n"
   ],
   "id": "recap-11"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Phase 7 Reporting Pipeline Status\n"
   ],
   "id": "recap-12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report_run = read_json(\"artifacts/reports/phase7/run_summary.json\", default={})\n",
    "manifest = read_json(\"artifacts/reports/phase7/provenance_manifest.json\", default={})\n",
    "traceability = read_json(\"artifacts/reports/phase7/report_traceability.json\", default={})\n",
    "\n",
    "print(\"Phase 7 run summary\")\n",
    "for k in [\"pipeline\", \"status\", \"started_at\", \"completed_at\", \"git_sha\"]:\n",
    "    print(f\"- {k}: {report_run.get(k)}\")\n",
    "\n",
    "failure = report_run.get(\"failure\") or {}\n",
    "if failure:\n",
    "    print()\n",
    "    print(\"Failure detail (if any)\")\n",
    "    for k in [\"stage\", \"status\", \"exit_code\", \"hint\", \"log\"]:\n",
    "        print(f\"- {k}: {failure.get(k)}\")\n",
    "\n",
    "sections = manifest.get(\"sections\", {})\n",
    "analysis_count = len(sections.get(\"analyses\", []))\n",
    "model_count = len(sections.get(\"models\", []))\n",
    "explainability_count = len(sections.get(\"explainability\", []))\n",
    "diagnostics_count = len(sections.get(\"diagnostics\", []))\n",
    "\n",
    "print()\n",
    "print(\"Manifest coverage\")\n",
    "print(f\"- analyses tracked: {analysis_count}\")\n",
    "print(f\"- models tracked: {model_count}\")\n",
    "print(f\"- explainability tracked: {explainability_count}\")\n",
    "print(f\"- diagnostics tracked: {diagnostics_count}\")\n",
    "\n",
    "trace_items = traceability.get(\"items\", {})\n",
    "print(f\"- traceability item count: {len(trace_items)}\")\n"
   ],
   "id": "recap-13"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Phase 8 Optimization and Serving Decision\n"
   ],
   "id": "recap-14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phase8_selected = sorted((ROOT / \"artifacts\" / \"models\").glob(\"phase8*/selected_model.json\"))\n",
    "rows = []\n",
    "for path in phase8_selected:\n",
    "    rel = path.relative_to(ROOT).as_posix()\n",
    "    payload = read_json(rel, default={})\n",
    "    metrics = payload.get(\"metrics\", {})\n",
    "    rows.append([\n",
    "        path.parent.name,\n",
    "        payload.get(\"best_model_family\"),\n",
    "        payload.get(\"winner_trial_id\"),\n",
    "        payload.get(\"winner_rule\"),\n",
    "        payload.get(\"accuracy_floor\"),\n",
    "        metrics.get(\"accuracy\"),\n",
    "        metrics.get(\"f1_macro\"),\n",
    "        payload.get(\"random_state\"),\n",
    "    ])\n",
    "\n",
    "print(\"Optimization runs with selected_model.json\")\n",
    "print_table(\n",
    "    [\n",
    "        \"run\",\n",
    "        \"family\",\n",
    "        \"winner_trial_id\",\n",
    "        \"winner_rule\",\n",
    "        \"acc_floor\",\n",
    "        \"accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"seed\",\n",
    "    ],\n",
    "    rows,\n",
    ")\n",
    "\n",
    "canonical = read_json(\"artifacts/models/phase8/selected_model.json\", default={})\n",
    "canon_metrics = canonical.get(\"metrics\", {})\n",
    "print()\n",
    "print(\"Canonical serving selection (artifacts/models/phase8/selected_model.json)\")\n",
    "print(f\"- best_model_family: {canonical.get('best_model_family')}\")\n",
    "print(f\"- winner_trial_id: {canonical.get('winner_trial_id')}\")\n",
    "print(f\"- winner_rule: {canonical.get('winner_rule')}\")\n",
    "print(f\"- accuracy_floor: {canonical.get('accuracy_floor')}\")\n",
    "print(f\"- accuracy: {canon_metrics.get('accuracy')}\")\n",
    "print(f\"- f1_macro: {canon_metrics.get('f1_macro')}\")\n",
    "\n",
    "seed_rows = [r for r in rows if r[0].startswith(\"phase8_seed\") and isinstance(r[6], float)]\n",
    "if seed_rows:\n",
    "    f1_values = [r[6] for r in seed_rows]\n",
    "    print()\n",
    "    print(f\"Seed-run macro-F1 mean: {statistics.mean(f1_values):.4f}\")\n",
    "    print(f\"Seed-run macro-F1 stdev: {statistics.pstdev(f1_values):.4f}\")\n"
   ],
   "id": "recap-15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Decision Log (What We Chose and Why)\n"
   ],
   "id": "recap-16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "case_study_text = (ROOT / \"docs\" / \"case_study.md\").read_text(encoding=\"utf-8\") if (ROOT / \"docs\" / \"case_study.md\").exists() else \"\"\n",
    "brief_text = (ROOT / \"docs\" / \"hiring_manager_brief.md\").read_text(encoding=\"utf-8\") if (ROOT / \"docs\" / \"hiring_manager_brief.md\").exists() else \"\"\n",
    "opt_text = (ROOT / \"docs\" / \"model_optimization_guide.md\").read_text(encoding=\"utf-8\") if (ROOT / \"docs\" / \"model_optimization_guide.md\").exists() else \"\"\n",
    "\n",
    "canonical = read_json(\"artifacts/models/phase8/selected_model.json\", default={})\n",
    "canon_metrics = canonical.get(\"metrics\", {})\n",
    "\n",
    "decisions = [\n",
    "    (\n",
    "        \"Script-first workflow\",\n",
    "        \"All executable logic moved to scripts/src and phase outputs are contract-tested.\",\n",
    "        \"Evidence: AGENTS.md + docs/analysis_workflow.md\",\n",
    "    ),\n",
    "    (\n",
    "        \"Use question [SEP] answer representation\",\n",
    "        \"Feature strategy ties question-answer context into one text stream for TF-IDF pipelines.\",\n",
    "        \"Evidence: AGENTS.md and docs/architecture.md\",\n",
    "    ),\n",
    "    (\n",
    "        \"Classical baseline emphasis first\",\n",
    "        \"Interpretable, fast baselines established stable benchmark behavior before advanced modeling.\",\n",
    "        \"Evidence: docs/case_study.md + docs/hiring_manager_brief.md\",\n",
    "    ),\n",
    "    (\n",
    "        \"Optimize with macro-F1 + accuracy floor\",\n",
    "        \"Selection policy protects minority-class quality while preventing unacceptable accuracy regression.\",\n",
    "        f\"Evidence: docs/model_optimization_guide.md + phase8 selected_model.json (floor={canonical.get('accuracy_floor')}, f1={canon_metrics.get('f1_macro')})\",\n",
    "    ),\n",
    "    (\n",
    "        \"Serve via canonical selected_model.json\",\n",
    "        \"Runtime resolution uses a deterministic selected-model file for reproducible deployment.\",\n",
    "        \"Evidence: docs/model_optimization_guide.md serving default section\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"Decisions and rationale\")\n",
    "for i, (decision, rationale, evidence) in enumerate(decisions, start=1):\n",
    "    print(f\"{i}. {decision}\")\n",
    "    print(f\"   - rationale: {rationale}\")\n",
    "    print(f\"   - {evidence}\")\n",
    "\n",
    "print()\n",
    "print(\"Cross-check: exact phrase presence in docs\")\n",
    "checks = [\n",
    "    (\"case_study_has_logreg_why\", \"Why Logistic Regression won\" in case_study_text),\n",
    "    (\"brief_has_tradeoffs\", \"Engineering Decisions and Tradeoffs\" in brief_text),\n",
    "    (\"opt_has_accuracy_floor\", \"Accuracy floor\" in opt_text),\n",
    "]\n",
    "for key, passed in checks:\n",
    "    print(f\"- {key}: {passed}\")\n"
   ],
   "id": "recap-17"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Final Recap\n"
   ],
   "id": "recap-18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phase5_summary = read_json(\"artifacts/models/phase5/run_summary.json\", default={})\n",
    "canonical = read_json(\"artifacts/models/phase8/selected_model.json\", default={})\n",
    "report_run = read_json(\"artifacts/reports/phase7/run_summary.json\", default={})\n",
    "label_diag = read_json(\"artifacts/diagnostics/phase6/label_diagnostics_summary.json\", default={})\n",
    "\n",
    "phase5_best = None\n",
    "if phase5_summary:\n",
    "    phase5_best = max(\n",
    "        phase5_summary.items(),\n",
    "        key=lambda kv: kv[1].get(\"f1_macro\", float(\"-inf\")),\n",
    "    )[0]\n",
    "\n",
    "recap = {\n",
    "    \"project\": \"EvasionBench\",\n",
    "    \"workflow\": \"script-first pipeline with artifact contracts\",\n",
    "    \"phase5_best_classical_family\": phase5_best,\n",
    "    \"phase8_canonical_family\": canonical.get(\"best_model_family\"),\n",
    "    \"phase8_canonical_trial\": canonical.get(\"winner_trial_id\"),\n",
    "    \"phase8_canonical_accuracy\": canonical.get(\"metrics\", {}).get(\"accuracy\"),\n",
    "    \"phase8_canonical_f1_macro\": canonical.get(\"metrics\", {}).get(\"f1_macro\"),\n",
    "    \"label_quality_score\": label_diag.get(\"quality_score\"),\n",
    "    \"phase7_report_status\": report_run.get(\"status\"),\n",
    "}\n",
    "\n",
    "print(\"Project recap summary\")\n",
    "for k, v in recap.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "print()\n",
    "print(\"Notebook complete: this run reconstructed work, outcomes, and decisions from repository evidence.\")\n"
   ],
   "id": "recap-19"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
