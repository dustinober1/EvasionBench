# Phase 9 Plan: GitHub Pages Excellence + Expanded Analysis

## Goal
Elevate the public site to portfolio-grade academic quality by improving trust signals, methodological clarity, and analytical depth while preserving script-first reproducibility.

## Why Phase 9 (Current Baseline)
Observed on the live site and pipeline:
- Summary cards can show fallback `unknown`/`n/a` values when optional artifacts are absent.
- Reproducibility can display failed run states that weaken reviewer confidence.
- Current pages emphasize artifact listing; they need stronger interpretation and narrative synthesis.
- Existing analysis coverage is strong for phases 3-6, but there is limited published error-analysis depth and limited exploratory views beyond current family outputs.

## Scope
In scope:
- Publication-quality site improvements (content + structure + credibility cues)
- Canonical metric source and publish-time quality gates
- New phase-9 error analysis and exploratory analysis artifacts
- Additional tests and CI checks for site quality

Out of scope:
- New model-family training loops
- Re-labeling program or annotation redesign
- Expensive external data acquisition that requires paid APIs

## Success Criteria (Phase-Level)
1. Site renders without placeholder metrics (`unknown`, `n/a`) for core KPI cards.
2. Home page clearly communicates abstract, contributions, and key quantitative findings.
3. New error-analysis page includes class-level confusion patterns and representative failure modes.
4. Reproducibility page shows latest successful run status and explicit provenance links.
5. At least two new exploratory slices are published (if data supports them), each with script outputs and tests.
6. Pages build remains under publication size budget and passes CI.

## Workstream A: Canonical Metrics + Trust Guardrails
### A1. Canonical KPI source
- Define one canonical metrics file for publication (`artifacts/publish/data/kpi_summary.json`).
- Populate from deterministic source priority:
  1) `artifacts/models/phase8/selected_model.json` + holdout metrics (if present)
  2) phase-5 model comparison summary
  3) manifest-derived fallback with explicit `provisional=true`

### A2. Publish-time quality gates
- Fail pages build if required KPI fields are missing for primary cards.
- Add a `site_quality_report.json` artifact with:
  - missing-fields list
  - fallback usage flags
  - run-status summary

### A3. Run-status hardening
- Reproducibility section must show:
  - latest successful run timestamp
  - latest attempted run status
  - link to log artifacts when failure exists

Deliverables:
- `scripts/build_pages_dataset.py` updates
- `artifacts/publish/data/kpi_summary.json` (generated)
- `artifacts/publish/data/site_quality_report.json` (generated)

Acceptance criteria:
- Core KPI cards never render placeholders in successful publish jobs.
- CI fails with actionable errors when KPI contract is violated.

## Workstream B: Academic Narrative and Information Architecture
### B1. Home page upgrade
Add high-signal sections in this order:
1. Abstract (5-7 lines)
2. Key contributions (3 bullets)
3. Primary results table
4. Practical implications + limitations snapshot

### B2. Methods clarity
- Add "Study Design" section with compact workflow figure and evaluation protocol.
- Add explicit assumptions and validity notes.

### B3. Findings synthesis
- For each major figure, add 1-2 sentence interpretation under it.
- Include direct links to backing artifacts and traceability IDs.

Deliverables:
- `scripts/render_github_pages.py` updates
- Updated generated pages under `artifacts/publish/site/`

Acceptance criteria:
- Every major visual has an interpretation caption (not just filename metadata).
- Home page tells a coherent narrative without opening other pages.

## Workstream C: Error Analysis (New Phase-9 Artifacts)
### C1. Build error-analysis script
Add `scripts/analyze_error_profiles.py` to generate:
- per-class precision/recall/F1 deltas vs baseline
- confusion concentration (top misclassification routes)
- hardest-class summary with evidence snippets (from available artifacts)

### C2. Artifact contract
Write to `artifacts/analysis/phase9/error_analysis/`:
- `error_summary.json`
- `misclassification_routes.csv`
- `hard_cases.md`
- `class_failure_heatmap.png`
- `artifact_index.json`

### C3. Site integration
- New page/section: "Error Analysis"
- Surface top-3 failure routes and implications for next iteration

Acceptance criteria:
- Error-analysis artifacts generated by script-only workflow.
- Site includes traceable links to all new phase-9 error artifacts.

## Workstream D: Expanded Exploratory Analysis
Run exploratory tracks only when required columns exist in prepared data.

### D1. Temporal slice (if date/quarter fields exist)
- Evasion rate over time
- model performance drift by period

### D2. Contextual slice (if sector/company fields exist)
- Evasion distribution by sector/category
- class confusion by segment

### D3. Question-intent slice
- Extend question behavior outputs with error alignment:
  - which question types most often trigger intermediate/fully_evasive confusion

Deliverables (example root):
- `artifacts/analysis/phase9/exploration/`
  - `temporal_summary.json`
  - `segment_summary.json`
  - `question_intent_error_map.csv`
  - `artifact_index.json`

Acceptance criteria:
- At least two exploratory outputs published, or explicit skip artifact documenting missing prerequisites.

## Workstream E: Reproducibility, Contracts, and CI
### E1. Contract tests
Add tests:
- `tests/test_pages_kpi_contract.py`
- `tests/test_phase9_error_analysis_contract.py`
- `tests/test_phase9_exploration_contract.py`

### E2. Site quality tests
- Validate no placeholder values on required KPI widgets.
- Validate page links resolve to published assets.
- Validate published artifact counts and size budget constraints.

### E3. CI wiring
- Keep Pages build deterministic and fail-fast on contract violations.
- Add build summary output to workflow logs for quick diagnosis.

Acceptance criteria:
- New tests pass locally and in CI.
- Pages workflow failures are informative and attributable to a contract violation.

## Implementation Sequence
1. Workstream A (KPI contract + guards)
2. Workstream B (home/methods/findings narrative)
3. Workstream C (error analysis generation + page integration)
4. Workstream D (exploratory analyses with conditional execution)
5. Workstream E (tests + CI hardening)

## Verification Loop (Per Workstream)
Use this cycle for each workstream before moving forward:
1. Implement script and artifact contract
2. Run targeted unit/contract tests
3. Run `python scripts/run_pages_pipeline.py --publish-root artifacts/publish --site-root artifacts/publish/site`
4. Validate generated JSON contract and rendered pages
5. Record outcome in commit message and move to next workstream

## Suggested Commands
```bash
# Core checks
black .
ruff check .
pytest -q

# Build site pipeline
python scripts/run_pages_pipeline.py \
  --publish-root artifacts/publish \
  --site-root artifacts/publish/site

# Optional local preview
python -m http.server 8000 --directory artifacts/publish/site
```

## Risks and Mitigations
- Risk: Missing optional artifacts or schema drift.
  - Mitigation: enforce explicit fallbacks + site quality report + contract tests.
- Risk: Overly large publish payload.
  - Mitigation: preserve allowlist and size budgets, fail-fast when exceeded.
- Risk: Exploratory data fields not present.
  - Mitigation: conditional scripts that emit explicit "skipped with reason" artifacts.
- Risk: Narrative drift from metrics.
  - Mitigation: generate all narrative metric snippets from canonical KPI JSON.

## Definition of Done
- Phase-9 artifacts and pages are produced entirely by scripts in `scripts/`.
- Core KPI cards are populated from canonical source and validated by tests.
- Error-analysis artifacts are published and linked from the site.
- At least two exploratory analyses are published or cleanly skipped with documented reason.
- GitHub Pages deploy passes with deterministic outputs and traceability intact.
