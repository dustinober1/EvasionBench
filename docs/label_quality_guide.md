# Label Quality Diagnostics Guide

This guide explains how to interpret and act on label quality diagnostics generated by Cleanlab for the EvasionBench dataset.

## Overview

Label quality diagnostics use **Cleanlab's Datalab** to identify potential issues in training data labels. These diagnostics help you:

- Detect potential **label errors** (examples where the given label may be incorrect)
- Identify **outliers** (examples that are unusual in feature space)
- Find **near-duplicates** (highly similar examples that may have inconsistent labels)
- Assess **class imbalance** (skewed class distributions)

The diagnostics run on **training data only** to prevent test data contamination.

## How Cleanlab Works

Cleanlab uses **confident learning** to identify label issues:

1. **Cross-validation**: Trains models on different subsets of training data
2. **Probabilistic labeling**: Estimates the probability of each true label
3. **Issue detection**: Identifies examples where predicted labels disagree with given labels
4. **Feature space analysis**: Detects outliers and near-duplicates using feature similarity

This approach is more robust than simple heuristics because it considers:
- Model uncertainty and confidence
- Feature space geometry
- Class-conditional probabilities

## Issue Types

### Label Error

**Definition**: Examples where the predicted label disagrees with the given label.

**Interpretation**:
- `label_score`: Probability that the given label is correct (lower = more suspect)
- Examples with `label_score < 0.5` are highly likely to be mislabeled
- Examples with `label_score < 0.7` should be reviewed

**Common causes**:
- Annotation mistakes (human error)
- Ambiguous examples (could reasonably have multiple labels)
- Edge cases (unusual patterns not well-represented in training data)

**Action**: Review the question and answer, determine if the label should be changed.

### Outlier

**Definition**: Examples that are far from the distribution of other examples in feature space.

**Interpretation**:
- `outlier_score`: Distance from typical examples (higher = more unusual)
- Outliers may be legitimate edge cases or data quality issues
- Not all outliers are errors - some may be valuable edge cases

**Common causes**:
- Unusual question/answer patterns
- Data quality issues (typos, formatting errors)
- Rare but legitimate examples

**Action**: Examine to determine if it's a valid edge case or data error.

### Near-Duplicate

**Definition**: Highly similar example pairs that may have inconsistent labels.

**Interpretation**:
- `distance_score`: Similarity between pairs (lower = more similar)
- Pairs with different labels suggest annotation inconsistency
- May indicate that the labeling guidelines are ambiguous

**Common causes**:
- Annotation inconsistency between different annotators
- Slight variations in the same example
- Ambiguous labeling criteria

**Action**: Ensure near-duplicates have consistent labels.

### Class Imbalance

**Definition**: Skewed distribution of examples across classes.

**Interpretation**:
- Affects model training (models may be biased toward majority class)
- May impact performance on minority class

**Action**: Consider class weighting, oversampling, or collecting more data for minority class.

## Understanding `label_score`

The `label_score` in Cleanlab output is the **probability that the given label is correct**.

| Score Range | Interpretation | Action |
|-------------|----------------|--------|
| 0.9 - 1.0 | Very confident label is correct | No action needed |
| 0.7 - 0.9 | Likely correct | Optional review |
| 0.5 - 0.7 | Suspect - may be incorrect | Review recommended |
| 0.0 - 0.5 | Very likely incorrect | Review strongly recommended |

**Note**: The threshold for "suspect" depends on your use case. For safety-critical applications, you may want to review all examples with `label_score < 0.9`.

## Review Workflow

### 1. Run Diagnostics

```bash
make label-diagnostics
```

Or via DVC:

```bash
dvc repro phase6_label_diagnostics
```

This generates artifacts in `artifacts/diagnostics/phase6/`:
- `suspect_examples.csv`: Examples with potential label issues
- `outlier_examples.csv`: Outlier examples
- `near_duplicate_pairs.csv`: Near-duplicate pairs
- `label_diagnostics_summary.json`: Issue counts and quality score
- `label_diagnostics_report.md`: Human-readable summary

### 2. Review Suspect Examples

Open `suspect_examples.csv` and sort by `label_score` (ascending):

```python
import pandas as pd

suspects = pd.read_csv("artifacts/diagnostics/phase6/suspect_examples.csv")
suspects_sorted = suspects.sort_values("label_score")
print(suspects_sorted[["question", "answer", "label", "label_score"]].head(20))
```

For each suspect example:
1. Read the question and answer
2. Determine if the label is correct
3. If incorrect, note the correct label
4. If ambiguous, consider if labeling guidelines need clarification

### 3. Correct Labels (If Needed)

After reviewing suspect examples, update the source data with corrected labels.

**IMPORTANT**: Make corrections in the **original dataset source**, not in the prepared parquet file. Re-run the data preparation pipeline to regenerate the prepared dataset.

```bash
# After correcting source data
make data-prepare
```

### 4. Re-run Diagnostics

After correcting labels, re-run diagnostics to verify improvements:

```bash
make label-diagnostics
```

Check that:
- Number of label issues has decreased
- Overall quality score has improved
- Previously suspect examples now have higher `label_score`

### 5. Re-train Models

With cleaned data, re-train models to potentially improve performance:

```bash
make model-phase5
make model-phase6
```

## Interpreting Diagnostic Summary

The `label_diagnostics_summary.json` contains key metrics:

```json
{
  "training_size": 800,
  "label_issues": 15,
  "outlier_issues": 8,
  "near_duplicate_issues": 12,
  "quality_score": 98.1,
  "random_state": 42
}
```

- **training_size**: Number of training examples (80% of dataset)
- **label_issues**: Number of examples with potential label errors
- **outlier_issues**: Number of outlier examples detected
- **near_duplicate_issues**: Number of near-duplicate pairs
- **quality_score**: Percentage of examples without label issues (higher is better)

### Quality Score Benchmarks

| Quality Score | Interpretation |
|---------------|----------------|
| 95% - 100% | Excellent - very few label issues |
| 90% - 95% | Good - some issues that should be reviewed |
| 80% - 90% | Fair - significant label noise, review recommended |
| < 80% | Poor - substantial label errors, cleaning strongly recommended |

## Limitations

1. **Feature dependence**: Diagnostics depend on the quality of TF-IDF features. If features don't capture meaningful patterns, diagnostics may be less reliable.

2. **False positives**: Not all flagged examples are actual errors. Some may be legitimate edge cases or ambiguous examples.

3. **Binary classification focus**: Current diagnostics are optimized for binary classification (evasive vs non-evasive).

4. **Model dependence**: Diagnostics use a LogisticRegression model as the basis. If this model is poorly specified, diagnostics may be less accurate.

5. **No test data leakage**: Diagnostics intentionally run on training data only. Test set quality is not assessed.

## Best Practices

1. **Iterative refinement**: Run diagnostics, review, correct, and re-run to iteratively improve data quality.

2. **Human-in-the-loop**: Always review suspect examples manually before changing labels. Diagnostics are a tool, not a replacement for human judgment.

3. **Track changes**: Keep a record of label corrections for reproducibility and analysis.

4. **Consider cost-benefit**: For small datasets, review all examples. For large datasets, prioritize low `label_score` examples.

5. **Update labeling guidelines**: If you find many ambiguous examples, update labeling guidelines to reduce ambiguity.

## Troubleshooting

### No issues detected

If diagnostics report no issues:
- May indicate high-quality labels (good!)
- Or may indicate the diagnostic model is too weak to detect issues
- Check that the dataset has enough variation and examples

### Too many issues detected

If diagnostics flag many examples as suspect:
- Check that labels are correctly encoded (evasive vs non_evasive)
- Verify that the feature extraction is working correctly
- Consider lowering the threshold for what counts as "suspect"

### Outliers seem normal

If flagged outliers appear to be normal examples:
- May indicate the feature space doesn't capture important distinctions
- Consider using different features or a different model for diagnostics

## References

- [Cleanlab Documentation](https://docs.cleanlab.ai/)
- [Confident Learning Paper](https://arxiv.org/abs/1911.02063)
- [Label Error Quantification](https://l7.curtisnorthcutt.com/label-errors)

---

**Last Updated**: 2025-02-10

**Questions or Issues**: Please open an issue on the EvasionBench repository.
